{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561ba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe85e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data generator. Its job is to generate new images out of existing images by rotating, zooming, shifting and flipping them\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    zoom_range=0.30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858a20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zygarde-50.png: 809/809/80998099\r"
     ]
    }
   ],
   "source": [
    "# Here we generate the data used to train the model\n",
    "X_data = []\n",
    "Y_data = []\n",
    "images_path = r'C:\\Users\\HP\\Desktop\\image classification and XMl on Pokemon dataset\\archive\\images\\images'\n",
    "images = os.listdir(images_path)\n",
    "\n",
    "i = 0\n",
    "images_count = len(images)\n",
    "for filename in images:\n",
    "    i += 1\n",
    "    print(f'{filename}: {i}/{images_count}', end='\\r')\n",
    "    # Load the image from the dataset\n",
    "    image = cv2.imread(os.path.join(images_path, filename))\n",
    "    # Resize it to 28x28 to make it faster to train\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    if resized_image is not None:\n",
    "        # Put the pixels inside X_data and the Pokémon name in Y_data\n",
    "        X_data.append(resized_image)\n",
    "        img_name = filename.split('.')[0]\n",
    "        Y_data.append(img_name)\n",
    "\n",
    "        # Generate 100 new images for the given Pokémon image by randoming rotating, zooming, shifting and flipping the image\n",
    "        for _ in range(100):\n",
    "            random_img = datagen.random_transform(image)\n",
    "            random_img = cv2.resize(random_img, (28, 28))\n",
    "            X_data.append(random_img)\n",
    "            Y_data.append(img_name)\n",
    "\n",
    "# Divide the pixels by 255 to make their range go from 0 until 1 instead of 0-255. \n",
    "# It makes the training faster\n",
    "X_data = np.array(X_data).astype('float32')\n",
    "X_data /= 255.0\n",
    "# Since we need numbers instead of words, we need to convert all the Pokémon names to numbers\n",
    "encoder = LabelEncoder()\n",
    "Y_data = encoder.fit_transform(Y_data)\n",
    "Y_data = to_categorical(Y_data)\n",
    "\n",
    "# Split the data into training and testing data, making 80% to train and 20% to validate how well we are really predicting the Pokémon names\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AI model \n",
    "# It uses a few convolution layers, a few normalization layers and a few dropout layers to avoid overfitting\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(128, kernel_size=4, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3a1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 6.0783 - accuracy: 0.0103\n",
      "Epoch 1: loss improved from inf to 6.07827, saving model to weights.h5\n",
      "340/340 [==============================] - 47s 95ms/step - loss: 6.0783 - accuracy: 0.0103 - val_loss: 12.5418 - val_accuracy: 0.0013 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 4.7394 - accuracy: 0.0455\n",
      "Epoch 2: loss improved from 6.07827 to 4.73945, saving model to weights.h5\n",
      "340/340 [==============================] - 40s 116ms/step - loss: 4.7394 - accuracy: 0.0455 - val_loss: 4.3184 - val_accuracy: 0.0742 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 3.8223 - accuracy: 0.1234\n",
      "Epoch 3: loss improved from 4.73945 to 3.82231, saving model to weights.h5\n",
      "340/340 [==============================] - 40s 116ms/step - loss: 3.8223 - accuracy: 0.1234 - val_loss: 7.2419 - val_accuracy: 0.0233 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 3.1560 - accuracy: 0.2199\n",
      "Epoch 4: loss improved from 3.82231 to 3.15598, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 116ms/step - loss: 3.1560 - accuracy: 0.2199 - val_loss: 3.4586 - val_accuracy: 0.1699 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 2.6884 - accuracy: 0.3099\n",
      "Epoch 5: loss improved from 3.15598 to 2.68840, saving model to weights.h5\n",
      "340/340 [==============================] - 40s 117ms/step - loss: 2.6884 - accuracy: 0.3099 - val_loss: 3.9163 - val_accuracy: 0.1515 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 2.3771 - accuracy: 0.3773\n",
      "Epoch 6: loss improved from 2.68840 to 2.37711, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 2.3771 - accuracy: 0.3773 - val_loss: 2.7087 - val_accuracy: 0.3017 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 2.1154 - accuracy: 0.4373\n",
      "Epoch 7: loss improved from 2.37711 to 2.11542, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 2.1154 - accuracy: 0.4373 - val_loss: 1.9958 - val_accuracy: 0.4229 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.9314 - accuracy: 0.4862\n",
      "Epoch 8: loss improved from 2.11542 to 1.93141, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 1.9314 - accuracy: 0.4862 - val_loss: 1.3620 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.7854 - accuracy: 0.5217\n",
      "Epoch 9: loss improved from 1.93141 to 1.78541, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 116ms/step - loss: 1.7854 - accuracy: 0.5217 - val_loss: 1.2223 - val_accuracy: 0.6262 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.6559 - accuracy: 0.5560\n",
      "Epoch 10: loss improved from 1.78541 to 1.65593, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 1.6559 - accuracy: 0.5560 - val_loss: 1.5205 - val_accuracy: 0.5595 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.5828\n",
      "Epoch 11: loss improved from 1.65593 to 1.54874, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 1.5487 - accuracy: 0.5828 - val_loss: 1.1114 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.4672 - accuracy: 0.6044\n",
      "Epoch 12: loss improved from 1.54874 to 1.46723, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 1.4672 - accuracy: 0.6044 - val_loss: 1.3747 - val_accuracy: 0.5911 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.3850 - accuracy: 0.6255\n",
      "Epoch 13: loss improved from 1.46723 to 1.38500, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 1.3850 - accuracy: 0.6255 - val_loss: 0.8802 - val_accuracy: 0.7173 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.3386 - accuracy: 0.6369\n",
      "Epoch 14: loss improved from 1.38500 to 1.33857, saving model to weights.h5\n",
      "340/340 [==============================] - 40s 118ms/step - loss: 1.3386 - accuracy: 0.6369 - val_loss: 3.4810 - val_accuracy: 0.3036 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.6460\n",
      "Epoch 15: loss improved from 1.33857 to 1.30503, saving model to weights.h5\n",
      "340/340 [==============================] - 38s 110ms/step - loss: 1.3050 - accuracy: 0.6460 - val_loss: 1.2169 - val_accuracy: 0.6205 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.2506 - accuracy: 0.6610\n",
      "Epoch 16: loss improved from 1.30503 to 1.25060, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.2506 - accuracy: 0.6610 - val_loss: 0.9332 - val_accuracy: 0.7062 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.2247 - accuracy: 0.6706\n",
      "Epoch 17: loss improved from 1.25060 to 1.22467, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 1.2247 - accuracy: 0.6706 - val_loss: 0.6076 - val_accuracy: 0.8066 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.1625 - accuracy: 0.6849\n",
      "Epoch 18: loss improved from 1.22467 to 1.16249, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 1.1625 - accuracy: 0.6849 - val_loss: 0.8238 - val_accuracy: 0.7296 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.1453 - accuracy: 0.6895\n",
      "Epoch 19: loss improved from 1.16249 to 1.14530, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 1.1453 - accuracy: 0.6895 - val_loss: 1.3727 - val_accuracy: 0.6010 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.6961\n",
      "Epoch 20: loss improved from 1.14530 to 1.11707, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 66ms/step - loss: 1.1171 - accuracy: 0.6961 - val_loss: 0.9089 - val_accuracy: 0.7136 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.1070 - accuracy: 0.6989\n",
      "Epoch 21: loss improved from 1.11707 to 1.10703, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.1070 - accuracy: 0.6989 - val_loss: 0.4526 - val_accuracy: 0.8597 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.0778 - accuracy: 0.7062\n",
      "Epoch 22: loss improved from 1.10703 to 1.07779, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.0778 - accuracy: 0.7062 - val_loss: 0.7048 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.0594 - accuracy: 0.7101\n",
      "Epoch 23: loss improved from 1.07779 to 1.05944, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.0594 - accuracy: 0.7101 - val_loss: 1.3030 - val_accuracy: 0.6169 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.0175 - accuracy: 0.7208\n",
      "Epoch 24: loss improved from 1.05944 to 1.01749, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.0175 - accuracy: 0.7208 - val_loss: 1.0899 - val_accuracy: 0.6660 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.0213 - accuracy: 0.7227\n",
      "Epoch 25: loss did not improve from 1.01749\n",
      "340/340 [==============================] - 21s 61ms/step - loss: 1.0213 - accuracy: 0.7227 - val_loss: 1.2262 - val_accuracy: 0.6352 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 1.0011 - accuracy: 0.7265\n",
      "Epoch 26: loss improved from 1.01749 to 1.00114, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 1.0011 - accuracy: 0.7265 - val_loss: 1.0214 - val_accuracy: 0.6992 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9720 - accuracy: 0.7362\n",
      "Epoch 27: loss improved from 1.00114 to 0.97196, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.9720 - accuracy: 0.7362 - val_loss: 0.5406 - val_accuracy: 0.8266 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9667 - accuracy: 0.7357\n",
      "Epoch 28: loss improved from 0.97196 to 0.96674, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.9667 - accuracy: 0.7357 - val_loss: 0.7883 - val_accuracy: 0.7651 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9426 - accuracy: 0.7421\n",
      "Epoch 29: loss improved from 0.96674 to 0.94259, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.9426 - accuracy: 0.7421 - val_loss: 0.8039 - val_accuracy: 0.7657 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.7478\n",
      "Epoch 30: loss improved from 0.94259 to 0.92143, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.9214 - accuracy: 0.7478 - val_loss: 0.7689 - val_accuracy: 0.7616 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9058 - accuracy: 0.7493\n",
      "Epoch 31: loss improved from 0.92143 to 0.90577, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.9058 - accuracy: 0.7493 - val_loss: 0.5090 - val_accuracy: 0.8344 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.9038 - accuracy: 0.7543\n",
      "Epoch 32: loss improved from 0.90577 to 0.90379, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.9038 - accuracy: 0.7543 - val_loss: 0.3044 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8887 - accuracy: 0.7575\n",
      "Epoch 33: loss improved from 0.90379 to 0.88869, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 68ms/step - loss: 0.8887 - accuracy: 0.7575 - val_loss: 0.4354 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8861 - accuracy: 0.7600\n",
      "Epoch 34: loss improved from 0.88869 to 0.88606, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 67ms/step - loss: 0.8861 - accuracy: 0.7600 - val_loss: 0.7068 - val_accuracy: 0.7743 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8668 - accuracy: 0.7637\n",
      "Epoch 35: loss improved from 0.88606 to 0.86678, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.8668 - accuracy: 0.7637 - val_loss: 0.5910 - val_accuracy: 0.8072 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8707 - accuracy: 0.7618\n",
      "Epoch 36: loss did not improve from 0.86678\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.8707 - accuracy: 0.7618 - val_loss: 0.7422 - val_accuracy: 0.7697 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8609 - accuracy: 0.7674\n",
      "Epoch 37: loss improved from 0.86678 to 0.86093, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.8609 - accuracy: 0.7674 - val_loss: 0.7812 - val_accuracy: 0.7488 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8572 - accuracy: 0.7665\n",
      "Epoch 38: loss improved from 0.86093 to 0.85723, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.8572 - accuracy: 0.7665 - val_loss: 0.6555 - val_accuracy: 0.7941 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8463 - accuracy: 0.7692\n",
      "Epoch 39: loss improved from 0.85723 to 0.84632, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.8463 - accuracy: 0.7692 - val_loss: 0.7594 - val_accuracy: 0.7625 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.7729\n",
      "Epoch 40: loss improved from 0.84632 to 0.82688, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.8269 - accuracy: 0.7729 - val_loss: 0.9413 - val_accuracy: 0.7308 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8321 - accuracy: 0.7751\n",
      "Epoch 41: loss did not improve from 0.82688\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.8321 - accuracy: 0.7751 - val_loss: 1.0228 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.8043 - accuracy: 0.7806\n",
      "Epoch 42: loss improved from 0.82688 to 0.80433, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.8043 - accuracy: 0.7806 - val_loss: 0.4486 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.7834\n",
      "Epoch 43: loss improved from 0.80433 to 0.79723, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.7972 - accuracy: 0.7834 - val_loss: 0.3905 - val_accuracy: 0.8740 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7940 - accuracy: 0.7839\n",
      "Epoch 44: loss improved from 0.79723 to 0.79398, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.7940 - accuracy: 0.7839 - val_loss: 0.6699 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7988 - accuracy: 0.7832\n",
      "Epoch 45: loss did not improve from 0.79398\n",
      "340/340 [==============================] - 23s 67ms/step - loss: 0.7988 - accuracy: 0.7832 - val_loss: 0.4071 - val_accuracy: 0.8651 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.7863\n",
      "Epoch 46: loss improved from 0.79398 to 0.78643, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 68ms/step - loss: 0.7864 - accuracy: 0.7863 - val_loss: 0.3239 - val_accuracy: 0.8892 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.7846\n",
      "Epoch 47: loss improved from 0.78643 to 0.78445, saving model to weights.h5\n",
      "340/340 [==============================] - 37s 108ms/step - loss: 0.7844 - accuracy: 0.7846 - val_loss: 0.5113 - val_accuracy: 0.8341 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7675 - accuracy: 0.7891\n",
      "Epoch 48: loss improved from 0.78445 to 0.76754, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.7675 - accuracy: 0.7891 - val_loss: 0.5049 - val_accuracy: 0.8435 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7768 - accuracy: 0.7872\n",
      "Epoch 49: loss did not improve from 0.76754\n",
      "340/340 [==============================] - 24s 71ms/step - loss: 0.7768 - accuracy: 0.7872 - val_loss: 0.9473 - val_accuracy: 0.7318 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7558 - accuracy: 0.7923\n",
      "Epoch 50: loss improved from 0.76754 to 0.75584, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.7558 - accuracy: 0.7923 - val_loss: 0.5924 - val_accuracy: 0.8173 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7586 - accuracy: 0.7925\n",
      "Epoch 51: loss did not improve from 0.75584\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.7586 - accuracy: 0.7925 - val_loss: 1.6878 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7634 - accuracy: 0.7913\n",
      "Epoch 52: loss did not improve from 0.75584\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.7634 - accuracy: 0.7913 - val_loss: 0.3774 - val_accuracy: 0.8724 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.7975\n",
      "Epoch 53: loss improved from 0.75584 to 0.73907, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 68ms/step - loss: 0.7391 - accuracy: 0.7975 - val_loss: 0.3287 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.7988\n",
      "Epoch 54: loss improved from 0.73907 to 0.72714, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.7271 - accuracy: 0.7988 - val_loss: 1.3883 - val_accuracy: 0.6270 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.8001\n",
      "Epoch 55: loss did not improve from 0.72714\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.7279 - accuracy: 0.8001 - val_loss: 0.4966 - val_accuracy: 0.8419 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.8017\n",
      "Epoch 56: loss improved from 0.72714 to 0.72499, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.7250 - accuracy: 0.8017 - val_loss: 0.6453 - val_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7255 - accuracy: 0.8026\n",
      "Epoch 57: loss did not improve from 0.72499\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.7255 - accuracy: 0.8026 - val_loss: 0.6718 - val_accuracy: 0.7913 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7247 - accuracy: 0.8025\n",
      "Epoch 58: loss improved from 0.72499 to 0.72468, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.7247 - accuracy: 0.8025 - val_loss: 0.2724 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.8074\n",
      "Epoch 59: loss improved from 0.72468 to 0.70849, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.7085 - accuracy: 0.8074 - val_loss: 0.3064 - val_accuracy: 0.8990 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.8070\n",
      "Epoch 60: loss did not improve from 0.70849\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.7128 - accuracy: 0.8070 - val_loss: 0.2490 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7100 - accuracy: 0.8047\n",
      "Epoch 61: loss did not improve from 0.70849\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.7100 - accuracy: 0.8047 - val_loss: 1.0600 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.8098\n",
      "Epoch 62: loss improved from 0.70849 to 0.70008, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.7001 - accuracy: 0.8098 - val_loss: 0.2859 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.8127\n",
      "Epoch 63: loss improved from 0.70008 to 0.68390, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.6839 - accuracy: 0.8127 - val_loss: 0.3507 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.8104\n",
      "Epoch 64: loss did not improve from 0.68390\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.6939 - accuracy: 0.8104 - val_loss: 0.5547 - val_accuracy: 0.8222 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.8124\n",
      "Epoch 65: loss did not improve from 0.68390\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.6898 - accuracy: 0.8125 - val_loss: 0.7009 - val_accuracy: 0.7949 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.8137\n",
      "Epoch 66: loss did not improve from 0.68390\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.6841 - accuracy: 0.8137 - val_loss: 0.7264 - val_accuracy: 0.7740 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.8129\n",
      "Epoch 67: loss improved from 0.68390 to 0.68067, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.6807 - accuracy: 0.8129 - val_loss: 0.3874 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.8179\n",
      "Epoch 68: loss improved from 0.68067 to 0.66790, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.6679 - accuracy: 0.8179 - val_loss: 0.5087 - val_accuracy: 0.8441 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6572 - accuracy: 0.8205\n",
      "Epoch 69: loss improved from 0.66790 to 0.65721, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6572 - accuracy: 0.8205 - val_loss: 0.4330 - val_accuracy: 0.8596 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.8186\n",
      "Epoch 70: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 23s 68ms/step - loss: 0.6632 - accuracy: 0.8186 - val_loss: 0.4467 - val_accuracy: 0.8691 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6716 - accuracy: 0.8165\n",
      "Epoch 71: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 24s 71ms/step - loss: 0.6716 - accuracy: 0.8165 - val_loss: 0.6962 - val_accuracy: 0.8096 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.8190\n",
      "Epoch 72: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 37s 109ms/step - loss: 0.6584 - accuracy: 0.8190 - val_loss: 0.6245 - val_accuracy: 0.8176 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6593 - accuracy: 0.8206\n",
      "Epoch 73: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 24s 71ms/step - loss: 0.6593 - accuracy: 0.8206 - val_loss: 0.3348 - val_accuracy: 0.8906 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.8158\n",
      "Epoch 74: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.6706 - accuracy: 0.8158 - val_loss: 8.9338 - val_accuracy: 0.1582 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6871 - accuracy: 0.8108\n",
      "Epoch 75: loss did not improve from 0.65721\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.6871 - accuracy: 0.8108 - val_loss: 0.2149 - val_accuracy: 0.9289 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.8217\n",
      "Epoch 76: loss improved from 0.65721 to 0.65107, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.6511 - accuracy: 0.8217 - val_loss: 0.3260 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.8233\n",
      "Epoch 77: loss improved from 0.65107 to 0.64982, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6498 - accuracy: 0.8233 - val_loss: 0.5795 - val_accuracy: 0.8194 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.8245\n",
      "Epoch 78: loss improved from 0.64982 to 0.64371, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6437 - accuracy: 0.8245 - val_loss: 0.4728 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.8258\n",
      "Epoch 79: loss improved from 0.64371 to 0.64193, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 66ms/step - loss: 0.6419 - accuracy: 0.8258 - val_loss: 0.4128 - val_accuracy: 0.8649 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6312 - accuracy: 0.8291\n",
      "Epoch 80: loss improved from 0.64193 to 0.63120, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6312 - accuracy: 0.8291 - val_loss: 0.2564 - val_accuracy: 0.9151 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.8293\n",
      "Epoch 81: loss improved from 0.63120 to 0.62498, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6250 - accuracy: 0.8293 - val_loss: 0.1353 - val_accuracy: 0.9544 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.8283\n",
      "Epoch 82: loss did not improve from 0.62498\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.6262 - accuracy: 0.8283 - val_loss: 0.2284 - val_accuracy: 0.9246 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.8269\n",
      "Epoch 83: loss did not improve from 0.62498\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6336 - accuracy: 0.8269 - val_loss: 0.5402 - val_accuracy: 0.8360 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.8281\n",
      "Epoch 84: loss improved from 0.62498 to 0.62039, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6204 - accuracy: 0.8281 - val_loss: 0.1972 - val_accuracy: 0.9359 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.8319\n",
      "Epoch 85: loss did not improve from 0.62039\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6225 - accuracy: 0.8319 - val_loss: 0.1919 - val_accuracy: 0.9350 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.8288\n",
      "Epoch 86: loss did not improve from 0.62039\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6213 - accuracy: 0.8288 - val_loss: 0.3856 - val_accuracy: 0.8860 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.8303\n",
      "Epoch 87: loss improved from 0.62039 to 0.61954, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6195 - accuracy: 0.8303 - val_loss: 0.2197 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6137 - accuracy: 0.8323\n",
      "Epoch 88: loss improved from 0.61954 to 0.61369, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.6137 - accuracy: 0.8323 - val_loss: 0.5704 - val_accuracy: 0.8409 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.8332\n",
      "Epoch 89: loss improved from 0.61369 to 0.61359, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6136 - accuracy: 0.8332 - val_loss: 0.4262 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.8338\n",
      "Epoch 90: loss did not improve from 0.61359\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6150 - accuracy: 0.8338 - val_loss: 0.4470 - val_accuracy: 0.8693 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.8312\n",
      "Epoch 91: loss improved from 0.61359 to 0.61354, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.6135 - accuracy: 0.8312 - val_loss: 0.3152 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.8366\n",
      "Epoch 92: loss improved from 0.61354 to 0.60012, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6001 - accuracy: 0.8366 - val_loss: 0.1097 - val_accuracy: 0.9623 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.8331\n",
      "Epoch 93: loss did not improve from 0.60012\n",
      "340/340 [==============================] - 22s 66ms/step - loss: 0.6096 - accuracy: 0.8331 - val_loss: 0.2220 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.8356\n",
      "Epoch 94: loss did not improve from 0.60012\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.6012 - accuracy: 0.8356 - val_loss: 0.1849 - val_accuracy: 0.9372 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.6073 - accuracy: 0.8327\n",
      "Epoch 95: loss did not improve from 0.60012\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.6073 - accuracy: 0.8327 - val_loss: 0.2152 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.8394\n",
      "Epoch 96: loss improved from 0.60012 to 0.59566, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 66ms/step - loss: 0.5957 - accuracy: 0.8394 - val_loss: 0.1908 - val_accuracy: 0.9386 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.8375\n",
      "Epoch 97: loss improved from 0.59566 to 0.58955, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 65ms/step - loss: 0.5896 - accuracy: 0.8375 - val_loss: 0.2085 - val_accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5820 - accuracy: 0.8405\n",
      "Epoch 98: loss improved from 0.58955 to 0.58201, saving model to weights.h5\n",
      "340/340 [==============================] - 23s 66ms/step - loss: 0.5820 - accuracy: 0.8405 - val_loss: 0.1599 - val_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5942 - accuracy: 0.8378\n",
      "Epoch 99: loss did not improve from 0.58201\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.5942 - accuracy: 0.8378 - val_loss: 0.2237 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.8376\n",
      "Epoch 100: loss did not improve from 0.58201\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.5937 - accuracy: 0.8376 - val_loss: 0.3962 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5917 - accuracy: 0.8394\n",
      "Epoch 101: loss did not improve from 0.58201\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.5917 - accuracy: 0.8394 - val_loss: 0.4933 - val_accuracy: 0.8471 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.8412\n",
      "Epoch 102: loss did not improve from 0.58201\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5869 - accuracy: 0.8412 - val_loss: 0.0898 - val_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.8410\n",
      "Epoch 103: loss improved from 0.58201 to 0.57744, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5774 - accuracy: 0.8410 - val_loss: 0.4106 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.8408\n",
      "Epoch 104: loss did not improve from 0.57744\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5826 - accuracy: 0.8408 - val_loss: 0.1464 - val_accuracy: 0.9509 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.8414\n",
      "Epoch 105: loss did not improve from 0.57744\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5783 - accuracy: 0.8414 - val_loss: 0.7384 - val_accuracy: 0.8025 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.8429\n",
      "Epoch 106: loss did not improve from 0.57744\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5783 - accuracy: 0.8429 - val_loss: 0.2589 - val_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8456\n",
      "Epoch 107: loss improved from 0.57744 to 0.56439, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5644 - accuracy: 0.8456 - val_loss: 0.2769 - val_accuracy: 0.9151 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.8444\n",
      "Epoch 108: loss did not improve from 0.56439\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5694 - accuracy: 0.8444 - val_loss: 0.2305 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.8407\n",
      "Epoch 109: loss did not improve from 0.56439\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5773 - accuracy: 0.8407 - val_loss: 0.5252 - val_accuracy: 0.8736 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5689 - accuracy: 0.8455\n",
      "Epoch 110: loss did not improve from 0.56439\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5689 - accuracy: 0.8455 - val_loss: 0.3627 - val_accuracy: 0.8798 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5726 - accuracy: 0.8433\n",
      "Epoch 111: loss did not improve from 0.56439\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5726 - accuracy: 0.8433 - val_loss: 0.2577 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5639 - accuracy: 0.8461\n",
      "Epoch 112: loss improved from 0.56439 to 0.56387, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5639 - accuracy: 0.8461 - val_loss: 0.4867 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.8454\n",
      "Epoch 113: loss did not improve from 0.56387\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5651 - accuracy: 0.8454 - val_loss: 0.4188 - val_accuracy: 0.8703 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.8472\n",
      "Epoch 114: loss improved from 0.56387 to 0.55595, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5559 - accuracy: 0.8472 - val_loss: 0.4293 - val_accuracy: 0.8725 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "339/340 [============================>.] - ETA: 0s - loss: 0.5538 - accuracy: 0.8481\n",
      "Epoch 115: loss improved from 0.55595 to 0.55410, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5541 - accuracy: 0.8480 - val_loss: 0.1577 - val_accuracy: 0.9463 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.8480\n",
      "Epoch 116: loss did not improve from 0.55410\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5545 - accuracy: 0.8480 - val_loss: 0.2302 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.8478\n",
      "Epoch 117: loss did not improve from 0.55410\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5615 - accuracy: 0.8478 - val_loss: 0.4412 - val_accuracy: 0.9191 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.8497\n",
      "Epoch 118: loss improved from 0.55410 to 0.54694, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5469 - accuracy: 0.8497 - val_loss: 0.4272 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.8469\n",
      "Epoch 119: loss did not improve from 0.54694\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5587 - accuracy: 0.8469 - val_loss: 0.4630 - val_accuracy: 0.8558 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5510 - accuracy: 0.8494\n",
      "Epoch 120: loss did not improve from 0.54694\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5510 - accuracy: 0.8494 - val_loss: 0.2944 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.8510\n",
      "Epoch 121: loss improved from 0.54694 to 0.54286, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5429 - accuracy: 0.8510 - val_loss: 0.1826 - val_accuracy: 0.9365 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.8524\n",
      "Epoch 122: loss improved from 0.54286 to 0.54178, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5418 - accuracy: 0.8524 - val_loss: 0.2973 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.8540\n",
      "Epoch 123: loss improved from 0.54178 to 0.53517, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5352 - accuracy: 0.8540 - val_loss: 0.3208 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.8518\n",
      "Epoch 124: loss did not improve from 0.53517\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5449 - accuracy: 0.8518 - val_loss: 0.1614 - val_accuracy: 0.9466 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.8533\n",
      "Epoch 125: loss did not improve from 0.53517\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5376 - accuracy: 0.8533 - val_loss: 0.2262 - val_accuracy: 0.9253 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.8535\n",
      "Epoch 126: loss did not improve from 0.53517\n",
      "340/340 [==============================] - 23s 67ms/step - loss: 0.5405 - accuracy: 0.8535 - val_loss: 0.3099 - val_accuracy: 0.9082 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.8523\n",
      "Epoch 127: loss did not improve from 0.53517\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5442 - accuracy: 0.8523 - val_loss: 0.4752 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.8516\n",
      "Epoch 128: loss did not improve from 0.53517\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5429 - accuracy: 0.8516 - val_loss: 0.3267 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.8533\n",
      "Epoch 129: loss improved from 0.53517 to 0.53454, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5345 - accuracy: 0.8533 - val_loss: 0.3204 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.8575\n",
      "Epoch 130: loss improved from 0.53454 to 0.52180, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.5218 - accuracy: 0.8575 - val_loss: 0.2955 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.8547\n",
      "Epoch 131: loss did not improve from 0.52180\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.5363 - accuracy: 0.8547 - val_loss: 0.6007 - val_accuracy: 0.8306 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.8554\n",
      "Epoch 132: loss did not improve from 0.52180\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.5303 - accuracy: 0.8554 - val_loss: 0.2249 - val_accuracy: 0.9255 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8544\n",
      "Epoch 133: loss did not improve from 0.52180\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5313 - accuracy: 0.8544 - val_loss: 0.5652 - val_accuracy: 0.8655 - lr: 0.0010\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.8547\n",
      "Epoch 134: loss did not improve from 0.52180\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5357 - accuracy: 0.8547 - val_loss: 0.1562 - val_accuracy: 0.9459 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.8564\n",
      "Epoch 135: loss did not improve from 0.52180\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5295 - accuracy: 0.8564 - val_loss: 0.0751 - val_accuracy: 0.9761 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.8578\n",
      "Epoch 136: loss improved from 0.52180 to 0.51697, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5170 - accuracy: 0.8578 - val_loss: 0.3677 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5419 - accuracy: 0.8529\n",
      "Epoch 137: loss did not improve from 0.51697\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.5419 - accuracy: 0.8529 - val_loss: 0.1153 - val_accuracy: 0.9611 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.8570\n",
      "Epoch 138: loss did not improve from 0.51697\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5236 - accuracy: 0.8570 - val_loss: 0.2883 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.8549\n",
      "Epoch 139: loss did not improve from 0.51697\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.5258 - accuracy: 0.8549 - val_loss: 0.2034 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5233 - accuracy: 0.8589\n",
      "Epoch 140: loss did not improve from 0.51697\n",
      "340/340 [==============================] - 29s 86ms/step - loss: 0.5233 - accuracy: 0.8589 - val_loss: 0.2431 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8589\n",
      "Epoch 141: loss did not improve from 0.51697\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5191 - accuracy: 0.8589 - val_loss: 0.0916 - val_accuracy: 0.9703 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.8595\n",
      "Epoch 142: loss improved from 0.51697 to 0.51209, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5121 - accuracy: 0.8595 - val_loss: 0.1831 - val_accuracy: 0.9422 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8599\n",
      "Epoch 143: loss improved from 0.51209 to 0.51198, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 72ms/step - loss: 0.5120 - accuracy: 0.8599 - val_loss: 0.1782 - val_accuracy: 0.9415 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.8577\n",
      "Epoch 144: loss did not improve from 0.51198\n",
      "340/340 [==============================] - 24s 71ms/step - loss: 0.5161 - accuracy: 0.8577 - val_loss: 0.2379 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.8578\n",
      "Epoch 145: loss did not improve from 0.51198\n",
      "340/340 [==============================] - 24s 72ms/step - loss: 0.5261 - accuracy: 0.8578 - val_loss: 0.1537 - val_accuracy: 0.9494 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.8589\n",
      "Epoch 146: loss did not improve from 0.51198\n",
      "340/340 [==============================] - 27s 80ms/step - loss: 0.5155 - accuracy: 0.8589 - val_loss: 0.2223 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.8595\n",
      "Epoch 147: loss did not improve from 0.51198\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5150 - accuracy: 0.8595 - val_loss: 0.1836 - val_accuracy: 0.9423 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.8622\n",
      "Epoch 148: loss improved from 0.51198 to 0.50934, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5093 - accuracy: 0.8622 - val_loss: 0.1615 - val_accuracy: 0.9477 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5083 - accuracy: 0.8602\n",
      "Epoch 149: loss improved from 0.50934 to 0.50834, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.5083 - accuracy: 0.8602 - val_loss: 0.8029 - val_accuracy: 0.8948 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.8582\n",
      "Epoch 150: loss did not improve from 0.50834\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5150 - accuracy: 0.8582 - val_loss: 0.1093 - val_accuracy: 0.9627 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.8604\n",
      "Epoch 151: loss did not improve from 0.50834\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.5112 - accuracy: 0.8604 - val_loss: 0.4460 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.8605\n",
      "Epoch 152: loss improved from 0.50834 to 0.50765, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.5077 - accuracy: 0.8605 - val_loss: 2.4970 - val_accuracy: 0.7254 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.8607\n",
      "Epoch 153: loss did not improve from 0.50765\n",
      "340/340 [==============================] - 25s 75ms/step - loss: 0.5095 - accuracy: 0.8607 - val_loss: 0.3230 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.8624\n",
      "Epoch 154: loss did not improve from 0.50765\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.5090 - accuracy: 0.8624 - val_loss: 0.2990 - val_accuracy: 0.9036 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5110 - accuracy: 0.8614\n",
      "Epoch 155: loss did not improve from 0.50765\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5110 - accuracy: 0.8614 - val_loss: 0.1076 - val_accuracy: 0.9637 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8623\n",
      "Epoch 156: loss improved from 0.50765 to 0.50365, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.5037 - accuracy: 0.8623 - val_loss: 0.1707 - val_accuracy: 0.9433 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.8634\n",
      "Epoch 157: loss improved from 0.50365 to 0.50054, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.5005 - accuracy: 0.8634 - val_loss: 0.1966 - val_accuracy: 0.9346 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.8625\n",
      "Epoch 158: loss did not improve from 0.50054\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5062 - accuracy: 0.8625 - val_loss: 0.2391 - val_accuracy: 0.9252 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.8632\n",
      "Epoch 159: loss improved from 0.50054 to 0.49944, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.4994 - accuracy: 0.8632 - val_loss: 0.1805 - val_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.8621\n",
      "Epoch 160: loss did not improve from 0.49944\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5077 - accuracy: 0.8621 - val_loss: 0.4787 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8644\n",
      "Epoch 161: loss did not improve from 0.49944\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5008 - accuracy: 0.8644 - val_loss: 0.3319 - val_accuracy: 0.8942 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.8640\n",
      "Epoch 162: loss did not improve from 0.49944\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5051 - accuracy: 0.8640 - val_loss: 0.2692 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.8573\n",
      "Epoch 163: loss did not improve from 0.49944\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.5264 - accuracy: 0.8573 - val_loss: 0.0825 - val_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.8623\n",
      "Epoch 164: loss did not improve from 0.49944\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.5002 - accuracy: 0.8623 - val_loss: 0.3551 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8676\n",
      "Epoch 165: loss improved from 0.49944 to 0.49472, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.4947 - accuracy: 0.8676 - val_loss: 0.1756 - val_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.8627\n",
      "Epoch 166: loss did not improve from 0.49472\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.4985 - accuracy: 0.8627 - val_loss: 0.1976 - val_accuracy: 0.9407 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8632\n",
      "Epoch 167: loss improved from 0.49472 to 0.49422, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.4942 - accuracy: 0.8632 - val_loss: 0.1750 - val_accuracy: 0.9424 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.8653\n",
      "Epoch 168: loss did not improve from 0.49422\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.4954 - accuracy: 0.8653 - val_loss: 0.3132 - val_accuracy: 0.8996 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8645\n",
      "Epoch 169: loss did not improve from 0.49422\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.4990 - accuracy: 0.8645 - val_loss: 0.3282 - val_accuracy: 0.8984 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8633\n",
      "Epoch 170: loss did not improve from 0.49422\n",
      "340/340 [==============================] - 30s 89ms/step - loss: 0.4990 - accuracy: 0.8633 - val_loss: 0.1526 - val_accuracy: 0.9466 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.8660\n",
      "Epoch 171: loss improved from 0.49422 to 0.48909, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4891 - accuracy: 0.8660 - val_loss: 0.1796 - val_accuracy: 0.9425 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.8658\n",
      "Epoch 172: loss did not improve from 0.48909\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4895 - accuracy: 0.8658 - val_loss: 0.2266 - val_accuracy: 0.9260 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.8671\n",
      "Epoch 173: loss did not improve from 0.48909\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4893 - accuracy: 0.8671 - val_loss: 0.2722 - val_accuracy: 0.9147 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.8669\n",
      "Epoch 174: loss improved from 0.48909 to 0.48598, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4860 - accuracy: 0.8669 - val_loss: 0.1861 - val_accuracy: 0.9402 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8706\n",
      "Epoch 175: loss improved from 0.48598 to 0.47518, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4752 - accuracy: 0.8706 - val_loss: 0.3823 - val_accuracy: 0.8806 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8678\n",
      "Epoch 176: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4810 - accuracy: 0.8678 - val_loss: 0.1474 - val_accuracy: 0.9499 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.8675\n",
      "Epoch 177: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.4858 - accuracy: 0.8675 - val_loss: 0.3078 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8690\n",
      "Epoch 178: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4830 - accuracy: 0.8690 - val_loss: 0.2392 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.8675\n",
      "Epoch 179: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4844 - accuracy: 0.8675 - val_loss: 0.3868 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.8672\n",
      "Epoch 180: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4833 - accuracy: 0.8672 - val_loss: 0.1657 - val_accuracy: 0.9448 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.8609\n",
      "Epoch 181: loss did not improve from 0.47518\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.5028 - accuracy: 0.8609 - val_loss: 0.4034 - val_accuracy: 0.9268 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.8716\n",
      "Epoch 182: loss improved from 0.47518 to 0.47406, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.4741 - accuracy: 0.8716 - val_loss: 0.1344 - val_accuracy: 0.9554 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.8699\n",
      "Epoch 183: loss did not improve from 0.47406\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4766 - accuracy: 0.8699 - val_loss: 0.2186 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.8687\n",
      "Epoch 184: loss did not improve from 0.47406\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.4751 - accuracy: 0.8687 - val_loss: 0.2309 - val_accuracy: 0.9266 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.8718\n",
      "Epoch 185: loss improved from 0.47406 to 0.47075, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4707 - accuracy: 0.8718 - val_loss: 0.2668 - val_accuracy: 0.9173 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.8708\n",
      "Epoch 186: loss did not improve from 0.47075\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4755 - accuracy: 0.8708 - val_loss: 0.1853 - val_accuracy: 0.9417 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.8694\n",
      "Epoch 187: loss did not improve from 0.47075\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.4749 - accuracy: 0.8694 - val_loss: 0.2755 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.8695\n",
      "Epoch 188: loss did not improve from 0.47075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 22s 64ms/step - loss: 0.4780 - accuracy: 0.8695 - val_loss: 0.0846 - val_accuracy: 0.9726 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8736\n",
      "Epoch 189: loss improved from 0.47075 to 0.46344, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4634 - accuracy: 0.8736 - val_loss: 0.1600 - val_accuracy: 0.9476 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8682\n",
      "Epoch 190: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4781 - accuracy: 0.8682 - val_loss: 0.0979 - val_accuracy: 0.9667 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.8698\n",
      "Epoch 191: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4792 - accuracy: 0.8698 - val_loss: 0.1203 - val_accuracy: 0.9572 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.8731\n",
      "Epoch 192: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4686 - accuracy: 0.8731 - val_loss: 0.1838 - val_accuracy: 0.9399 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4767 - accuracy: 0.8695\n",
      "Epoch 193: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4767 - accuracy: 0.8695 - val_loss: 0.2807 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.8724\n",
      "Epoch 194: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.4682 - accuracy: 0.8724 - val_loss: 0.0755 - val_accuracy: 0.9747 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.8721\n",
      "Epoch 195: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4689 - accuracy: 0.8721 - val_loss: 0.1442 - val_accuracy: 0.9529 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4711 - accuracy: 0.8717\n",
      "Epoch 196: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4711 - accuracy: 0.8717 - val_loss: 0.4732 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.8731\n",
      "Epoch 197: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4709 - accuracy: 0.8731 - val_loss: 0.2484 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.8725\n",
      "Epoch 198: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4676 - accuracy: 0.8725 - val_loss: 0.1646 - val_accuracy: 0.9473 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.8718\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 199: loss did not improve from 0.46344\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4720 - accuracy: 0.8718 - val_loss: 0.3333 - val_accuracy: 0.8970 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8876\n",
      "Epoch 200: loss improved from 0.46344 to 0.41344, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4134 - accuracy: 0.8876 - val_loss: 0.1034 - val_accuracy: 0.9676 - lr: 2.0000e-04\n",
      "Epoch 201/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8923\n",
      "Epoch 201: loss improved from 0.41344 to 0.39689, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3969 - accuracy: 0.8923 - val_loss: 0.0819 - val_accuracy: 0.9758 - lr: 2.0000e-04\n",
      "Epoch 202/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.8915\n",
      "Epoch 202: loss did not improve from 0.39689\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.4013 - accuracy: 0.8915 - val_loss: 0.0800 - val_accuracy: 0.9745 - lr: 2.0000e-04\n",
      "Epoch 203/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8910\n",
      "Epoch 203: loss did not improve from 0.39689\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4045 - accuracy: 0.8910 - val_loss: 0.0823 - val_accuracy: 0.9745 - lr: 2.0000e-04\n",
      "Epoch 204/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.4008 - accuracy: 0.8926\n",
      "Epoch 204: loss did not improve from 0.39689\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.4008 - accuracy: 0.8926 - val_loss: 0.0900 - val_accuracy: 0.9719 - lr: 2.0000e-04\n",
      "Epoch 205/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8943\n",
      "Epoch 205: loss improved from 0.39689 to 0.38761, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3876 - accuracy: 0.8943 - val_loss: 0.0783 - val_accuracy: 0.9754 - lr: 2.0000e-04\n",
      "Epoch 206/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.8941\n",
      "Epoch 206: loss did not improve from 0.38761\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3944 - accuracy: 0.8941 - val_loss: 0.0883 - val_accuracy: 0.9723 - lr: 2.0000e-04\n",
      "Epoch 207/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8950\n",
      "Epoch 207: loss did not improve from 0.38761\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3904 - accuracy: 0.8950 - val_loss: 0.0802 - val_accuracy: 0.9748 - lr: 2.0000e-04\n",
      "Epoch 208/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.8954\n",
      "Epoch 208: loss improved from 0.38761 to 0.38463, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3846 - accuracy: 0.8954 - val_loss: 0.1096 - val_accuracy: 0.9644 - lr: 2.0000e-04\n",
      "Epoch 209/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8972\n",
      "Epoch 209: loss improved from 0.38463 to 0.38324, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3832 - accuracy: 0.8972 - val_loss: 0.1044 - val_accuracy: 0.9680 - lr: 2.0000e-04\n",
      "Epoch 210/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8950\n",
      "Epoch 210: loss did not improve from 0.38324\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3896 - accuracy: 0.8950 - val_loss: 0.1033 - val_accuracy: 0.9674 - lr: 2.0000e-04\n",
      "Epoch 211/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.8957\n",
      "Epoch 211: loss did not improve from 0.38324\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3897 - accuracy: 0.8957 - val_loss: 0.1120 - val_accuracy: 0.9647 - lr: 2.0000e-04\n",
      "Epoch 212/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8956\n",
      "Epoch 212: loss improved from 0.38324 to 0.38218, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.3822 - accuracy: 0.8956 - val_loss: 0.0765 - val_accuracy: 0.9768 - lr: 2.0000e-04\n",
      "Epoch 213/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.8956\n",
      "Epoch 213: loss did not improve from 0.38218\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.3876 - accuracy: 0.8956 - val_loss: 0.0763 - val_accuracy: 0.9763 - lr: 2.0000e-04\n",
      "Epoch 214/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8950\n",
      "Epoch 214: loss did not improve from 0.38218\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3910 - accuracy: 0.8950 - val_loss: 0.1234 - val_accuracy: 0.9605 - lr: 2.0000e-04\n",
      "Epoch 215/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8965\n",
      "Epoch 215: loss did not improve from 0.38218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3863 - accuracy: 0.8965 - val_loss: 0.0806 - val_accuracy: 0.9742 - lr: 2.0000e-04\n",
      "Epoch 216/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8948\n",
      "Epoch 216: loss did not improve from 0.38218\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3901 - accuracy: 0.8948 - val_loss: 0.0955 - val_accuracy: 0.9701 - lr: 2.0000e-04\n",
      "Epoch 217/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.8955\n",
      "Epoch 217: loss did not improve from 0.38218\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3880 - accuracy: 0.8955 - val_loss: 0.1049 - val_accuracy: 0.9660 - lr: 2.0000e-04\n",
      "Epoch 218/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.8958\n",
      "Epoch 218: loss did not improve from 0.38218\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3852 - accuracy: 0.8958 - val_loss: 0.0841 - val_accuracy: 0.9727 - lr: 2.0000e-04\n",
      "Epoch 219/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8970\n",
      "Epoch 219: loss improved from 0.38218 to 0.38016, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3802 - accuracy: 0.8970 - val_loss: 0.1091 - val_accuracy: 0.9655 - lr: 2.0000e-04\n",
      "Epoch 220/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8966\n",
      "Epoch 220: loss did not improve from 0.38016\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3848 - accuracy: 0.8966 - val_loss: 0.0818 - val_accuracy: 0.9744 - lr: 2.0000e-04\n",
      "Epoch 221/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8978\n",
      "Epoch 221: loss did not improve from 0.38016\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3811 - accuracy: 0.8978 - val_loss: 0.1031 - val_accuracy: 0.9673 - lr: 2.0000e-04\n",
      "Epoch 222/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3826 - accuracy: 0.8978\n",
      "Epoch 222: loss did not improve from 0.38016\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3826 - accuracy: 0.8978 - val_loss: 0.0943 - val_accuracy: 0.9703 - lr: 2.0000e-04\n",
      "Epoch 223/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.8953\n",
      "Epoch 223: loss did not improve from 0.38016\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3862 - accuracy: 0.8953 - val_loss: 0.0853 - val_accuracy: 0.9720 - lr: 2.0000e-04\n",
      "Epoch 224/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.8987\n",
      "Epoch 224: loss improved from 0.38016 to 0.37857, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3786 - accuracy: 0.8987 - val_loss: 0.1056 - val_accuracy: 0.9662 - lr: 2.0000e-04\n",
      "Epoch 225/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8946\n",
      "Epoch 225: loss did not improve from 0.37857\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3894 - accuracy: 0.8946 - val_loss: 0.0891 - val_accuracy: 0.9719 - lr: 2.0000e-04\n",
      "Epoch 226/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8999\n",
      "Epoch 226: loss improved from 0.37857 to 0.36529, saving model to weights.h5\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3653 - accuracy: 0.8999 - val_loss: 0.0873 - val_accuracy: 0.9719 - lr: 2.0000e-04\n",
      "Epoch 227/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8981\n",
      "Epoch 227: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3779 - accuracy: 0.8981 - val_loss: 0.0758 - val_accuracy: 0.9758 - lr: 2.0000e-04\n",
      "Epoch 228/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.9002\n",
      "Epoch 228: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3728 - accuracy: 0.9002 - val_loss: 0.0761 - val_accuracy: 0.9766 - lr: 2.0000e-04\n",
      "Epoch 229/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8978\n",
      "Epoch 229: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3806 - accuracy: 0.8978 - val_loss: 0.0894 - val_accuracy: 0.9714 - lr: 2.0000e-04\n",
      "Epoch 230/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.8953\n",
      "Epoch 230: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3861 - accuracy: 0.8953 - val_loss: 0.0701 - val_accuracy: 0.9769 - lr: 2.0000e-04\n",
      "Epoch 231/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.8999\n",
      "Epoch 231: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3745 - accuracy: 0.8999 - val_loss: 0.0893 - val_accuracy: 0.9720 - lr: 2.0000e-04\n",
      "Epoch 232/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8978\n",
      "Epoch 232: loss did not improve from 0.36529\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3757 - accuracy: 0.8978 - val_loss: 0.0845 - val_accuracy: 0.9739 - lr: 2.0000e-04\n",
      "Epoch 233/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.9010\n",
      "Epoch 233: loss improved from 0.36529 to 0.36391, saving model to weights.h5\n",
      "340/340 [==============================] - 22s 63ms/step - loss: 0.3639 - accuracy: 0.9010 - val_loss: 0.0738 - val_accuracy: 0.9776 - lr: 2.0000e-04\n",
      "Epoch 234/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9002\n",
      "Epoch 234: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 21s 62ms/step - loss: 0.3765 - accuracy: 0.9002 - val_loss: 0.0650 - val_accuracy: 0.9792 - lr: 2.0000e-04\n",
      "Epoch 235/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8985\n",
      "Epoch 235: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3770 - accuracy: 0.8985 - val_loss: 0.0728 - val_accuracy: 0.9775 - lr: 2.0000e-04\n",
      "Epoch 236/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3689 - accuracy: 0.9005\n",
      "Epoch 236: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 22s 64ms/step - loss: 0.3689 - accuracy: 0.9005 - val_loss: 0.0885 - val_accuracy: 0.9727 - lr: 2.0000e-04\n",
      "Epoch 237/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8980\n",
      "Epoch 237: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3807 - accuracy: 0.8980 - val_loss: 0.0910 - val_accuracy: 0.9706 - lr: 2.0000e-04\n",
      "Epoch 238/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.8983\n",
      "Epoch 238: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 21s 63ms/step - loss: 0.3751 - accuracy: 0.8983 - val_loss: 0.0802 - val_accuracy: 0.9745 - lr: 2.0000e-04\n",
      "Epoch 239/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8989\n",
      "Epoch 239: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 29s 85ms/step - loss: 0.3770 - accuracy: 0.8989 - val_loss: 0.0998 - val_accuracy: 0.9674 - lr: 2.0000e-04\n",
      "Epoch 240/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.9001\n",
      "Epoch 240: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.3692 - accuracy: 0.9001 - val_loss: 0.1040 - val_accuracy: 0.9660 - lr: 2.0000e-04\n",
      "Epoch 241/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.9009\n",
      "Epoch 241: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.3696 - accuracy: 0.9009 - val_loss: 0.0901 - val_accuracy: 0.9703 - lr: 2.0000e-04\n",
      "Epoch 242/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.8988\n",
      "Epoch 242: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 25s 73ms/step - loss: 0.3738 - accuracy: 0.8988 - val_loss: 0.0933 - val_accuracy: 0.9699 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8980\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 243: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 24s 71ms/step - loss: 0.3785 - accuracy: 0.8980 - val_loss: 0.0739 - val_accuracy: 0.9774 - lr: 2.0000e-04\n",
      "Epoch 244/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.9027\n",
      "Epoch 244: loss did not improve from 0.36391\n",
      "340/340 [==============================] - 25s 72ms/step - loss: 0.3639 - accuracy: 0.9027 - val_loss: 0.0861 - val_accuracy: 0.9731 - lr: 4.0000e-05\n",
      "Epoch 245/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.9022\n",
      "Epoch 245: loss improved from 0.36391 to 0.35906, saving model to weights.h5\n",
      "340/340 [==============================] - 25s 74ms/step - loss: 0.3591 - accuracy: 0.9022 - val_loss: 0.0851 - val_accuracy: 0.9734 - lr: 4.0000e-05\n",
      "Epoch 246/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.9017\n",
      "Epoch 246: loss did not improve from 0.35906\n",
      "340/340 [==============================] - 24s 72ms/step - loss: 0.3659 - accuracy: 0.9017 - val_loss: 0.0885 - val_accuracy: 0.9723 - lr: 4.0000e-05\n",
      "Epoch 247/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.9014\n",
      "Epoch 247: loss did not improve from 0.35906\n",
      "340/340 [==============================] - 40s 118ms/step - loss: 0.3639 - accuracy: 0.9014 - val_loss: 0.0748 - val_accuracy: 0.9761 - lr: 4.0000e-05\n",
      "Epoch 248/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.9006\n",
      "Epoch 248: loss did not improve from 0.35906\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3670 - accuracy: 0.9006 - val_loss: 0.0796 - val_accuracy: 0.9749 - lr: 4.0000e-05\n",
      "Epoch 249/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.9042\n",
      "Epoch 249: loss improved from 0.35906 to 0.35868, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3587 - accuracy: 0.9042 - val_loss: 0.0792 - val_accuracy: 0.9752 - lr: 4.0000e-05\n",
      "Epoch 250/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9034\n",
      "Epoch 250: loss improved from 0.35868 to 0.35631, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3563 - accuracy: 0.9034 - val_loss: 0.0816 - val_accuracy: 0.9752 - lr: 4.0000e-05\n",
      "Epoch 251/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.9033\n",
      "Epoch 251: loss did not improve from 0.35631\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3603 - accuracy: 0.9033 - val_loss: 0.0780 - val_accuracy: 0.9760 - lr: 4.0000e-05\n",
      "Epoch 252/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.9023\n",
      "Epoch 252: loss did not improve from 0.35631\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3647 - accuracy: 0.9023 - val_loss: 0.0722 - val_accuracy: 0.9777 - lr: 4.0000e-05\n",
      "Epoch 253/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.9033\n",
      "Epoch 253: loss did not improve from 0.35631\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3623 - accuracy: 0.9033 - val_loss: 0.0724 - val_accuracy: 0.9772 - lr: 4.0000e-05\n",
      "Epoch 254/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.9048\n",
      "Epoch 254: loss did not improve from 0.35631\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3588 - accuracy: 0.9048 - val_loss: 0.0765 - val_accuracy: 0.9766 - lr: 4.0000e-05\n",
      "Epoch 255/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.9033\n",
      "Epoch 255: loss did not improve from 0.35631\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3570 - accuracy: 0.9033 - val_loss: 0.0670 - val_accuracy: 0.9793 - lr: 4.0000e-05\n",
      "Epoch 256/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.9054\n",
      "Epoch 256: loss improved from 0.35631 to 0.35257, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3526 - accuracy: 0.9054 - val_loss: 0.0722 - val_accuracy: 0.9783 - lr: 4.0000e-05\n",
      "Epoch 257/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.9032\n",
      "Epoch 257: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3555 - accuracy: 0.9032 - val_loss: 0.0745 - val_accuracy: 0.9764 - lr: 4.0000e-05\n",
      "Epoch 258/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.9033\n",
      "Epoch 258: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3581 - accuracy: 0.9033 - val_loss: 0.0788 - val_accuracy: 0.9752 - lr: 4.0000e-05\n",
      "Epoch 259/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3550 - accuracy: 0.9050\n",
      "Epoch 259: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3550 - accuracy: 0.9050 - val_loss: 0.0733 - val_accuracy: 0.9769 - lr: 4.0000e-05\n",
      "Epoch 260/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.9014\n",
      "Epoch 260: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3637 - accuracy: 0.9014 - val_loss: 0.0730 - val_accuracy: 0.9778 - lr: 4.0000e-05\n",
      "Epoch 261/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.9039\n",
      "Epoch 261: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3602 - accuracy: 0.9039 - val_loss: 0.0678 - val_accuracy: 0.9783 - lr: 4.0000e-05\n",
      "Epoch 262/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.9026\n",
      "Epoch 262: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3641 - accuracy: 0.9026 - val_loss: 0.0716 - val_accuracy: 0.9766 - lr: 4.0000e-05\n",
      "Epoch 263/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.9034\n",
      "Epoch 263: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3577 - accuracy: 0.9034 - val_loss: 0.0703 - val_accuracy: 0.9780 - lr: 4.0000e-05\n",
      "Epoch 264/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9055\n",
      "Epoch 264: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3563 - accuracy: 0.9055 - val_loss: 0.0718 - val_accuracy: 0.9778 - lr: 4.0000e-05\n",
      "Epoch 265/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.9028\n",
      "Epoch 265: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3620 - accuracy: 0.9028 - val_loss: 0.0739 - val_accuracy: 0.9771 - lr: 4.0000e-05\n",
      "Epoch 266/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.9020\n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 266: loss did not improve from 0.35257\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3665 - accuracy: 0.9020 - val_loss: 0.0677 - val_accuracy: 0.9783 - lr: 4.0000e-05\n",
      "Epoch 267/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.9062\n",
      "Epoch 267: loss improved from 0.35257 to 0.35038, saving model to weights.h5\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3504 - accuracy: 0.9062 - val_loss: 0.0730 - val_accuracy: 0.9771 - lr: 8.0000e-06\n",
      "Epoch 268/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.9046\n",
      "Epoch 268: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3529 - accuracy: 0.9046 - val_loss: 0.0712 - val_accuracy: 0.9772 - lr: 8.0000e-06\n",
      "Epoch 269/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.9053\n",
      "Epoch 269: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3526 - accuracy: 0.9053 - val_loss: 0.0711 - val_accuracy: 0.9775 - lr: 8.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.9047\n",
      "Epoch 270: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3536 - accuracy: 0.9047 - val_loss: 0.0697 - val_accuracy: 0.9782 - lr: 8.0000e-06\n",
      "Epoch 271/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.9043\n",
      "Epoch 271: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3525 - accuracy: 0.9043 - val_loss: 0.0699 - val_accuracy: 0.9779 - lr: 8.0000e-06\n",
      "Epoch 272/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.9027\n",
      "Epoch 272: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3613 - accuracy: 0.9027 - val_loss: 0.0704 - val_accuracy: 0.9778 - lr: 8.0000e-06\n",
      "Epoch 273/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.9031\n",
      "Epoch 273: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3606 - accuracy: 0.9031 - val_loss: 0.0743 - val_accuracy: 0.9764 - lr: 8.0000e-06\n",
      "Epoch 274/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.9032\n",
      "Epoch 274: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3566 - accuracy: 0.9032 - val_loss: 0.0717 - val_accuracy: 0.9777 - lr: 8.0000e-06\n",
      "Epoch 275/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.9032\n",
      "Epoch 275: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3598 - accuracy: 0.9032 - val_loss: 0.0694 - val_accuracy: 0.9781 - lr: 8.0000e-06\n",
      "Epoch 276/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.9066\n",
      "Epoch 276: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3520 - accuracy: 0.9066 - val_loss: 0.0709 - val_accuracy: 0.9774 - lr: 8.0000e-06\n",
      "Epoch 277/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.9046\n",
      "Epoch 277: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 277: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3528 - accuracy: 0.9046 - val_loss: 0.0743 - val_accuracy: 0.9763 - lr: 8.0000e-06\n",
      "Epoch 278/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9055\n",
      "Epoch 278: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3517 - accuracy: 0.9055 - val_loss: 0.0736 - val_accuracy: 0.9769 - lr: 1.6000e-06\n",
      "Epoch 279/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3546 - accuracy: 0.9057\n",
      "Epoch 279: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3546 - accuracy: 0.9057 - val_loss: 0.0706 - val_accuracy: 0.9778 - lr: 1.6000e-06\n",
      "Epoch 280/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.9051\n",
      "Epoch 280: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3519 - accuracy: 0.9051 - val_loss: 0.0717 - val_accuracy: 0.9772 - lr: 1.6000e-06\n",
      "Epoch 281/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.9027\n",
      "Epoch 281: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3587 - accuracy: 0.9027 - val_loss: 0.0712 - val_accuracy: 0.9774 - lr: 1.6000e-06\n",
      "Epoch 282/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.9048\n",
      "Epoch 282: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3544 - accuracy: 0.9048 - val_loss: 0.0718 - val_accuracy: 0.9771 - lr: 1.6000e-06\n",
      "Epoch 283/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.9040\n",
      "Epoch 283: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3597 - accuracy: 0.9040 - val_loss: 0.0734 - val_accuracy: 0.9766 - lr: 1.6000e-06\n",
      "Epoch 284/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.9025\n",
      "Epoch 284: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3618 - accuracy: 0.9025 - val_loss: 0.0720 - val_accuracy: 0.9774 - lr: 1.6000e-06\n",
      "Epoch 285/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.9032\n",
      "Epoch 285: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 115ms/step - loss: 0.3579 - accuracy: 0.9032 - val_loss: 0.0719 - val_accuracy: 0.9772 - lr: 1.6000e-06\n",
      "Epoch 286/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.9056\n",
      "Epoch 286: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3509 - accuracy: 0.9056 - val_loss: 0.0723 - val_accuracy: 0.9771 - lr: 1.6000e-06\n",
      "Epoch 287/500\n",
      "340/340 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.9034\n",
      "Epoch 287: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 287: loss did not improve from 0.35038\n",
      "340/340 [==============================] - 39s 114ms/step - loss: 0.3623 - accuracy: 0.9034 - val_loss: 0.0737 - val_accuracy: 0.9769 - lr: 1.6000e-06\n",
      "Epoch 287: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the AI using the image data generator. \n",
    "# It uses 500 iterations, but we probably won't reach that because each iteraction has a EarlyStopping callback, allowing the training to stop early if we are not making any more progress\n",
    "model.fit(datagen.flow(X_train, Y_train, batch_size=192),\n",
    "          epochs=500,\n",
    "          steps_per_epoch=X_train.shape[0] // 192,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[EarlyStopping(monitor='loss', min_delta=1e-10, patience=20, verbose=1),\n",
    "                     ReduceLROnPlateau(monitor='loss', factor=0.2, patience=10, verbose=1),\n",
    "                     ModelCheckpoint(filepath='weights.h5', monitor='loss',\n",
    "                                     save_best_only=True, verbose=1)])\n",
    "# Save the model so we don't have to train again (download the file if you want to test it later)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9faf81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Specify the path to the saved model file (.h5 or .tf format)\n",
    "model_path = r'C:\\Users\\HP\\Desktop\\image classification and XMl on Pokemon dataset\\model.h5'  # Replace with your model's path\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# You can now use the loaded_model for predictions or further operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d259ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 32)        25632     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 10, 10, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          102464    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4, 4, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 1, 1, 128)         131200    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 1, 1, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 809)               104361    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 430,889\n",
      "Trainable params: 430,057\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbc1d75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the testing data (the 20% we separated earlier) and the training data we used to train the model. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# It will output something close to 0.97 (97% accuracy)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mX_test\u001b[49m, Y_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation)\n\u001b[0;32m      5\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, Y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the testing data (the 20% we separated earlier) and the training data we used to train the model. \n",
    "# It will output something close to 0.97 (97% accuracy)\n",
    "evaluation = model.evaluate(X_test, Y_test)\n",
    "print(evaluation)\n",
    "evaluation = model.evaluate(X_train, Y_train)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c51b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d\n",
      "batch_normalization\n",
      "conv2d_1\n",
      "batch_normalization_1\n",
      "conv2d_2\n",
      "batch_normalization_2\n",
      "dropout\n",
      "conv2d_3\n",
      "batch_normalization_3\n",
      "conv2d_4\n",
      "batch_normalization_4\n",
      "conv2d_5\n",
      "batch_normalization_5\n",
      "dropout_1\n",
      "conv2d_6\n",
      "batch_normalization_6\n",
      "flatten\n",
      "dropout_2\n",
      "dense\n"
     ]
    }
   ],
   "source": [
    "# List all the layers in your model\n",
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "065f1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications.inception_v3 import decode_predictions\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "model = load_model('model.h5')\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(28, 28))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "test_image_path = r\"C:\\Users\\HP\\Desktop\\image classification and XMl on Pokemon dataset\\archive\\images\\images\\bulbasaur.png\" # Replace with the path to your test image\n",
    "test_image = load_and_preprocess_image(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c8f837c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 24, 24, 32) dtype=float32 (created by layer 'conv2d_1')>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name = 'conv2d_1'\n",
    "last_conv_layer = model.get_layer(layer_name)\n",
    "last_conv_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14fdc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, x, layer_name):\n",
    "    preds = model.predict(x)\n",
    "    class_idx = np.argmax(preds[0])\n",
    "    class_output = model.output[:, class_idx]\n",
    "    last_conv_layer = model.get_layer(layer_name)\n",
    "    \n",
    "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "    \n",
    "    for i in range(pooled_grads_value.shape[0]):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    \n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31850c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, x, layer_name):\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer = model.get_layer(layer_name)\n",
    "        iterate = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output])\n",
    "        \n",
    "        model_out, last_conv_layer = iterate(x)\n",
    "        class_idx = tf.argmax(model_out[0])\n",
    "        loss = model_out[:, class_idx]\n",
    "        \n",
    "    grads = tape.gradient(loss, last_conv_layer)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    heatmap /= tf.reduce_max(heatmap)\n",
    "    \n",
    "    return heatmap.numpy()\n",
    "\n",
    "gradcam_heatmap = grad_cam(model, test_image, layer_name)\n",
    "\n",
    "# SmoothGrad\n",
    "def smooth_grad(model, x, layer_name, n=50, sigma=1.0):\n",
    "    heatmaps = []\n",
    "    for _ in range(n):\n",
    "        noise = tf.random.normal(shape=x.shape, mean=0.0, stddev=sigma)\n",
    "        noisy_input = x + noise\n",
    "        heatmap = grad_cam(model, noisy_input, layer_name)\n",
    "        heatmaps.append(heatmap)\n",
    "    return np.mean(heatmaps, axis=0)\n",
    "\n",
    "smoothgrad_heatmap = smooth_grad(model, test_image, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85b4b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "layer_name = 'conv2d_1'  # Replace with the name of the layer you want to visualize\n",
    "\n",
    "gradcam_heatmap = grad_cam(model, test_image, layer_name)\n",
    "smoothgrad_heatmap = smooth_grad(model, test_image, layer_name)\n",
    "\n",
    "# Rescale the heatmaps to the input image size\n",
    "gradcam_heatmap = cv2.resize(gradcam_heatmap, (28, 28))\n",
    "smoothgrad_heatmap = cv2.resize(smoothgrad_heatmap, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9b1657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAFKCAYAAACAbckCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABx8klEQVR4nO3deZwdVZ3//1ftdZfelyQEshEgJAGRIIqACIKMguKCOm4juC/jLN+v6Dj+VPy6MKi4DCOj4zjKDOAoin4RdXAWVBS+oux7gCxk7aT37rvVdn5/3O4mTRKophM64Pvp487Q935u1am6dW/qU6fO+VjGGIOIiIiIiMgs2HPdABEREREReeZTYiEiIiIiIrOmxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1JRYiIiIiIjJrSixERERERGTWlFiIiIiIiMisKbEQEREREZFZU2IhIvI0Ou+881iyZMlcN0NEnkEsy+LP//zP57oZM6bfuz8+SixE5I/C+vXr+fM//3MOP/xwisUixWKRlStX8oEPfIC77rprrpu3V6Ojo3zqU5/iOc95DuVymUKhwOrVq/nIRz7C1q1b9/ie17/+9ViWxUc+8pE9vv7LX/4Sy7KwLIsrrrhijzEnnngilmWxevXqJ23jeeedR7lc3uvrT8dJ0WWXXcZ3vvOd/boOeWa4++67Offcc1m8eDFhGLJw4ULOOOMMLr300rlu2hO66aabuPDCCxkeHp71sp5pv3cvfvGL9/pbs2HDBizL4otf/OJ+bcPnPvc5fvzjH+/XdfwxUGIhIs961113HatXr+bf/u3fOP300/nyl7/MV7/6VV72spfxs5/9jGOOOYaNGzfOdTN3s27dOo455hg+/elPs3LlSi6++GL+/u//nlNPPZVvfetbvPjFL97tPaOjo/zkJz9hyZIlfPe738UYs9flh2HIVVddtdvzGzZs4KabbiIMw325OfuVEguB5sn5cccdx5133sm73vUu/uEf/oF3vvOd2LbNV7/61blu3hO66aab+NSnPjXrxOKZ+ns315RY7BvuXDdARGR/euSRR/jTP/1TFi9ezH//93+zYMGCaa9ffPHFXHbZZdj2E19nqVQqlEql/dnUaZIk4TWveQ19fX388pe/5KSTTpr2+mc/+1kuvvji3d73wx/+kDRN+Zd/+RdOO+00fv3rX3PKKafscR0vf/nLufbaa+nv76e7u3vq+auuuop58+Zx2GGHMTQ0tG83TGQ/+uxnP0tbWxu///3vaW9vn/bajh075qZRT6Nn6u+dPHuox0JEntU+//nPU6lU+Pa3v73bP7IAruvyF3/xFxxyyCFTz03e2vPII4/w8pe/nJaWFt785jcDcOONN/K6172ORYsWEQQBhxxyCH/9139NrVbbbdk//vGPWb16NWEYsnr1an70ox/lbvcPf/hD7rzzTj72sY/tllQAtLa28tnPfna356+88krOOOMMTj31VI488kiuvPLKva7jnHPOIQgCrr766mnPX3XVVbz+9a/HcZzc7Z2pRqPBJz/5SZYvXz61Hz/84Q/TaDSmxX3729/mtNNOo7e3lyAIWLlyJf/4j/84LWbJkiXce++9/OpXv5q6xWuyN+c73/kOlmXxm9/8hr/4i7+gp6eH9vZ23vOe9xBFEcPDw/zZn/0ZHR0ddHR08OEPf3i3Xp4vfvGLvPCFL6Srq4tCocCaNWv4wQ9+sNs2Td7ydeWVV3LEEUcQhiFr1qzh17/+9b7debJXjzzyCKtWrdotqQDo7e2d9vfk53X11VezcuVKCoUCJ5xwAnfffTcA3/jGN1i+fDlhGPLiF7+YDRs27LbMq6++mjVr1lAoFOju7uYtb3kLW7Zs2S3uf/7nfzj55JMplUq0t7dzzjnncP/990+9fuGFF3LBBRcAsHTp0qnj+PHrnPxNCYKAVatW8R//8R/TXn+m/t49FcPDw/zVX/0VhxxyCEEQsHz5ci6++GKyLJsWl+f7a1kWlUqFyy+/fGrfn3feeUDzs7Esi7Vr1/KWt7yFtrY2enp6+PjHP44xhk2bNnHOOefQ2trK/PnzueSSS6YtO4oiPvGJT7BmzRra2toolUqcfPLJ3HDDDdPidr3l68tf/jKLFy+mUChwyimncM899+z7HbifqMdCRJ7VrrvuOpYvX87zn//8Gb0vSRLOPPNMTjrpJL74xS9SLBaB5olEtVrlfe97H11dXdxyyy1ceumlbN68edoJ+i9+8Qte+9rXsnLlSi666CIGBgY4//zzOfjgg3Ot/9prrwXgrW99a+42b926lRtuuIHLL78cgDe+8Y18+ctf5h/+4R/wfX+3+GKxyDnnnMN3v/td3ve+9wFw5513cu+99/LP//zPM74Xu7+/P1dclmW88pWv5De/+Q3vfve7OfLII7n77rv58pe/zNq1a6fdjvCP//iPrFq1ile+8pW4rstPfvIT3v/+95NlGR/4wAcA+MpXvsIHP/hByuUyH/vYxwCYN2/etHV+8IMfZP78+XzqU5/i//2//8c//dM/0d7ezk033cSiRYv43Oc+x89+9jO+8IUvsHr1av7sz/5s6r1f/epXeeUrX8mb3/xmoiji3//933nd617Hddddx1lnnTVtPb/61a/43ve+x1/8xV8QBAGXXXYZf/Inf8Itt9ySa7yKzM7ixYu5+eabueeee3Lt7xtvvJFrr7126li66KKLOPvss/nwhz/MZZddxvvf/36Ghob4/Oc/z9vf/nb+53/+Z+q93/nOdzj//PN53vOex0UXXURfXx9f/epX+e1vf8vtt98+ldz813/9Fy972ctYtmwZF154IbVajUsvvZQTTzyR2267jSVLlvCa17yGtWvX8t3vfpcvf/nLUz2IPT09U+v7zW9+wzXXXMP73/9+Wlpa+Pu//3te+9rX8uijj9LV1QU8c3/vANI03eNvyJ56TavVKqeccgpbtmzhPe95D4sWLeKmm27iox/9KNu2beMrX/nKVGye7++//du/8c53vpPjjz+ed7/73QAceuih09b5hje8gSOPPJK/+7u/46c//Smf+cxn6Ozs5Bvf+AannXYaF198MVdeeSUf+tCHeN7znseLXvQioHl76j//8z/zxje+kXe9612MjY3xrW99izPPPJNbbrmFY445Ztp6/vVf/5WxsTE+8IEPUK/X+epXv8ppp53G3Xffvdvv2gHJiIg8S42MjBjAvOpVr9rttaGhIbNz586pR7VanXrtbW97mwHM3/zN3+z2vl3jJl100UXGsiyzcePGqeeOOeYYs2DBAjM8PDz13C9+8QsDmMWLFz9p25/73Oeatra2J43b1Re/+EVTKBTM6OioMcaYtWvXGsD86Ec/mhZ3ww03GMBcffXV5rrrrjOWZZlHH33UGGPMBRdcYJYtW2aMMeaUU04xq1atetL1Tu6vJ3p84AMfmIr/t3/7N2PbtrnxxhunLefrX/+6Acxvf/vbqef2tL/PPPPMqTZOWrVqlTnllFN2i/32t79tAHPmmWeaLMumnj/hhBOMZVnmve9979RzSZKYgw8+eLflPL4NURSZ1atXm9NOO23a85Pb+oc//GHquY0bN5owDM2rX/3q3dom+94vfvEL4ziOcRzHnHDCCebDH/6wuf76600URbvFAiYIArN+/fqp577xjW8YwMyfP3/qe2SMMR/96EcNMBUbRZHp7e01q1evNrVabSruuuuuM4D5xCc+MfXcMcccY3p7e83AwMDUc3feeaexbdv82Z/92dRzX/jCF6at4/Ft9X3fPPzww9OWAZhLL73UGPPM/r075ZRTnvQ35Atf+MJU/Kc//WlTKpXM2rVrpy3nb/7mb4zjOFO/Z3vahr19f0ulknnb2962W9s++clPGsC8+93vnnpu8rfCsizzd3/3d1PPDw0NmUKhMG05SZKYRqMxbZlDQ0Nm3rx55u1vf/vUc+vXrzeAKRQKZvPmzVPP/+53vzOA+eu//us97boDjm6FEpFnrdHRUYA9zlj04he/mJ6enqnH1772td1iJq/i76pQKEz9d6VSob+/nxe+8IUYY7j99tsB2LZtG3fccQdve9vbaGtrm4o/44wzWLlyZe62t7S05IqddOWVV3LWWWdNve+www5jzZo1T3g71Etf+lI6Ozv593//d4wx/Pu//ztvfOMbZ7ReaA4E/8///M89Ph7v6quv5sgjj2TFihX09/dPPU477TSAabcI7Lq/R0ZG6O/v55RTTmHdunWMjIzkbt873vEOLMua+vv5z38+xhje8Y53TD3nOA7HHXcc69atm/beXdswNDTEyMgIJ598Mrfddttu6znhhBNYs2bN1N+LFi3inHPO4frrrydN09ztlafmjDPO4Oabb+aVr3wld955J5///Oc588wzWbhw4VQv4K5e8pKXTJsOdfJK/2tf+9pp37/J5yePjT/84Q/s2LGD97///dMmOTjrrLNYsWIFP/3pT4HHfgvOO+88Ojs7p+KOPvpozjjjDH72s5/l3rbTTz992lX0o48+mtbW1qk2PZN/76B5S+Oefj/2NHPd1Vdfzcknn0xHR8e035DTTz+dNE2n3X44k+/vE3nnO9859d+TvxWP/w1pb2/niCOOmPYb4jjOVI9xlmUMDg6SJAnHHXfcHtvwqle9ioULF079ffzxx/P85z9/RsfKXNKtUCLyrDV5YjA+Pr7ba9/4xjcYGxujr6+Pt7zlLbu97rruHrvxH330UT7xiU9w7bXX7tZFP3miOznjymGHHbbb+4844ohp/5js3Llz2glnuVymXC5PO2HI4/777+f222/nz/7sz3j44Yennn/xi1/M1772NUZHR2ltbd3tfZ7n8brXvY6rrrqK448/nk2bNvGmN70p93onOY7D6aefniv2oYce4v777592m8eudh1k+9vf/pZPfvKT3HzzzVSr1WlxIyMj005knsiiRYum/T35vl3vNZ98/vGf63XXXcdnPvMZ7rjjjmljQHZNVCbt6TM//PDDqVar7Ny5k/nz5+dqrzx1z3ve87jmmmuIoog777yTH/3oR3z5y1/m3HPP5Y477ph2sjuT4wIeuy1n8jt+xBFH7Lb+FStW8Jvf/OZJ44488kiuv/763AOlH99WgI6Ojqk2PRN+755IqVTa42/Insa2PPTQQ9x11125fkNm8v19Ins6VsIwnDbxxeTzAwMD0567/PLLueSSS3jggQeI43jq+aVLl+62nr39hnz/+9+fUXvnihILEXnWamtrY8GCBXsc+DZ5BXJP/2gBBEGw28wpaZpyxhlnMDg4yEc+8hFWrFhBqVRiy5YtnHfeebsNGszjec973rSpHz/5yU9y4YUXsmLFCm6//XY2bdq020nOnkxe1fvrv/5r/vqv/3q313/4wx9y/vnn7/G9b3rTm/j617/OhRdeyHOe85wZXWV8KrIs46ijjuJLX/rSHl+f3N5HHnmEl7zkJaxYsYIvfelLHHLIIfi+z89+9jO+/OUvz2h/720g+p6eN7sM3r7xxht55StfyYte9CIuu+wyFixYgOd5fPvb397jVL1y4PB9n+c973k873nP4/DDD+f888/n6quv5pOf/ORUzEyOC+AJp2/e356sTc+E37t9JcsyzjjjDD784Q/v8fXDDz8c2Lff3z3t/zzHyRVXXMF5553Hq171Ki644AJ6e3txHIeLLrqIRx55ZEZteCZQYiEiz2pnnXUW//zP/8wtt9zC8ccfP6tl3X333axdu5bLL7982uDex9/us3jxYqB5Ve3xHnzwwWl/X3nlldNmWFm2bBkAr3jFK/jud7/LFVdcwUc/+tEnbJcxhquuuopTTz2V97///bu9/ulPf5orr7xyr4nFSSedxKJFi/jlL3+5xyls97VDDz2UO++8k5e85CVPeNXwJz/5CY1Gg2uvvXba1cLHz6YCM7/6mNcPf/hDwjDk+uuvJwiCqee//e1v7zF+T5/52rVrKRaLe726KvvfcccdBzRv29kXJr/jDz744NQtfJMefPDBqdd3jXu8Bx54gO7u7qnein1xDB/ov3f7yqGHHsr4+PiT9pLO5Pu7v35DfvCDH7Bs2TKuueaaaevYNcHd1d5+Q54pFcw1xkJEntU+/OEPUywWefvb305fX99ur8/kCuTk1ald32OM2a3w1oIFCzjmmGO4/PLLp40D+M///E/uu+++abEnnngip59++tRjMrE499xzOeqoo/jsZz/LzTffvFtbxsbGpmZA+u1vf8uGDRs4//zzOffcc3d7vOENb+CGG27Ya6Vuy7L4+7//ez75yU/OaBaqp+r1r389W7Zs4Zvf/OZur9VqNSqVCrDn/T0yMrLHk4JSqbRPKhY/nuM4WJY17Xa1DRs27LWQ1s033zzt1o9Nmzbxf//v/+WlL33pfp2+V5puuOGGPX6nJ+9P39MtSU/FcccdR29vL1//+ten3V7z85//nPvvv39qtqFdfwt2PT7vuecefvGLX/Dyl7986rnJBGM2x/GB/nu3r7z+9a/n5ptv5vrrr9/tteHhYZIkAWb2/d2fvyEwfT/+7ne/2+PvOjSn7d11yuJbbrmF3/3ud7zsZS/b523bH9RjISLPaocddhhXXXUVb3zjGzniiCN485vfzHOe8xyMMaxfv56rrroK27ZzTYu4YsUKDj30UD70oQ+xZcsWWltb+eEPf7jH6RAvuugizjrrLE466STe/va3Mzg4yKWXXsqqVav2eA/043mexzXXXMPpp5/Oi170Il7/+tdz4okn4nke9957L1dddRUdHR189rOf5corr8RxnN2mPp30yle+ko997GP8+7//O//rf/2vPcacc845nHPOOU/arn3hrW99K9///vd573vfyw033MCJJ55ImqY88MADfP/73+f666/nuOOO46UvfSm+7/OKV7yC97znPYyPj/PNb36T3t7e3a48r1mzhn/8x3/kM5/5DMuXL6e3t3e3K8lPxVlnncWXvvQl/uRP/oQ3velN7Nixg6997WssX758j9Pxrl69mjPPPHPadLMAn/rUp2bdFnlyH/zgB6lWq7z61a9mxYoVRFHETTfdxPe+9z2WLFmy1167mfI8j4svvpjzzz+fU045hTe+8Y1T080uWbJk2u2IX/jCF3jZy17GCSecwDve8Y6p6Wbb2tq48MILp+ImB/1/7GMf40//9E/xPI9XvOIVMypU90z9vZupCy64gGuvvZazzz6b8847jzVr1lCpVLj77rv5wQ9+wIYNG+ju7p7R93fNmjX813/9F1/60pc46KCDWLp06Yyn7d2Ts88+m2uuuYZXv/rVnHXWWaxfv56vf/3rrFy5co/7Zvny5Zx00km8733vo9Fo8JWvfIWurq693vZ1wHla56ASEZkjDz/8sHnf+95nli9fbsIwNIVCwaxYscK8973vNXfccce02Le97W2mVCrtcTn33XefOf300025XDbd3d3mXe9619S0j9/+9renxf7whz80Rx55pAmCwKxcudJcc8015m1ve1uu6RcnDQ0NmU984hPmqKOOMsVi0YRhaFavXm0++tGPmm3btpkoikxXV5c5+eSTn3A5S5cuNc997nONMdOnm30iM5ludm/7yxiz23SzxjSnfLz44ovNqlWrTBAEpqOjw6xZs8Z86lOfMiMjI1Nx1157rTn66KNNGIZmyZIl5uKLLzb/8i//stu0nNu3bzdnnXWWaWlpMcDUlLGT083+/ve/n7b+ySkkd+7c+aTb8q1vfcscdthhJggCs2LFCvPtb3976v172s4rrrhiKv65z32uueGGG55sF8o+8vOf/9y8/e1vNytWrDDlctn4vm+WL19uPvjBD5q+vr5psXs6Lien/Nx1alNj9v6d+d73vmee+9znmiAITGdnp3nzm988barQSf/1X/9lTjzxRFMoFExra6t5xSteYe67777d4j796U+bhQsXGtu2px3je2qrMcYsXrx4j1OkPtN+757ot2Zvn8nY2Jj56Ec/apYvX2583zfd3d3mhS98ofniF784bXrhvN/fBx54wLzoRS8yhULBAFP7dSa/FXvalizLzOc+9zmzePHiqd+E6667brd9s+t2XnLJJeaQQw4xQRCYk08+2dx5551Pug8PFJYxczgSSURE5FnCsiw+8IEP8A//8A9z3RQReYbZsGEDS5cu5Qtf+AIf+tCH5ro5T5nGWIiIiIiIyKwpsRARERERkVlTYiEiIiIiIrOmMRYiIiIiIjJr6rEQEREREZFZU2IhIiIiIiKzpsRiDlx44YVPuXT8d77zHSzLYsOGDfu2UbvYsGEDlmXxne98Z7+tQ0RERESeXVR5ewbuvfdeLrroIm644Qb6+/vp6uri1FNP5W//9m9ZtWrVXDfvaffLX/6SU089lauvvppzzz13rpsjc+A1r/nAXDdBRPaDa6752lN632vO/2y+wMoMFprmjMs7YjTvdT1TzRkI4OQLs4OccTNYdc5FUs8Zl/fMsNGfMxBo6c4X155zeTM5fgbzHkCNnHFh/nUXcn6Qtd2rmc9u3TM5gLxcUddc8/F9vuY/atdccw3HHnss//3f/83555/PZZddxjve8Q5uuOEGjj32WH70ox/lXtb/9//9f9RqtafUjre+9a3UajUWL178lN4vIiIiIrI/qMcih0ceeYS3vvWtLFu2jF//+tf09PRMvfaXf/mXnHzyybz1rW/lrrvuYtmyZXtdTqVSoVQq4bourvvUdr3jODhOzisjIiIiIiJPE/VY5PCFL3yBarXKP/3TP01LKgC6u7v5xje+QaVS4fOf//zU85PjKO677z7e9KY30dHRwUknnTTttV3VajX+4i/+gu7ublpaWnjlK1/Jli1bsCyLCy+8cCpuT2MslixZwtlnn81vfvMbjj/+eMIwZNmyZfzrv/7rtHUMDg7yoQ99iKOOOopyuUxraysve9nLuPPOO/fRnnps29auXctb3vIW2tra6Onp4eMf/zjGGDZt2sQ555xDa2sr8+fP55JLLpn2/iiK+MQnPsGaNWtoa2ujVCpx8sknc8MNN+y2roGBAd761rfS2tpKe3s7b3vb27jzzjv3OD7kgQce4Nxzz6Wzs5MwDDnuuOO49tpr99l2i4iIiPyxU2KRw09+8hOWLFnCySefvMfXX/SiF7FkyRJ++tOf7vba6173OqrVKp/73Od417vetdd1nHfeeVx66aW8/OUv5+KLL6ZQKHDWWWflbuPDDz/MueeeyxlnnMEll1xCR0cH5513Hvfee+9UzLp16/jxj3/M2WefzZe+9CUuuOAC7r77bk455RS2bt2ae115vOENbyDLMv7u7/6O5z//+XzmM5/hK1/5CmeccQYLFy7k4osvZvny5XzoQx/i17/+9dT7RkdH+ed//mde/OIXc/HFF3PhhReyc+dOzjzzTO64446puCzLeMUrXsF3v/td3va2t/HZz36Wbdu28ba3vW23ttx777284AUv4P777+dv/uZvuOSSSyiVSrzqVa+a0S1sIiIiIrJ3uhXqSYyMjLB161bOOeecJ4w7+uijufbaaxkbG6OlpWXq+ec85zlcddVVT/je2267je9///v81V/9FV/+8pcBeP/738/555+fuzfhwQcf5Ne//vVU8vP617+eQw45hG9/+9t88YtfBOCoo45i7dq12PZj+eRb3/pWVqxYwbe+9S0+/vF8A3PyOP744/nGN74BwLvf/W6WLFnC//7f/5uLLrqIj3zkIwC88Y1v5KCDDuJf/uVfeNGLXgRAR0cHGzZswPf9qWW9613vYsWKFVx66aV861vfAuDHP/4xN998M1/5ylf4y7/8SwDe9773ccYZZ+zWlr/8y79k0aJF/P73vycImqPc3v/+93PSSSfxkY98hFe/+tX7bLtFRERE/lipx+JJjI2NAUxLFvZk8vXR0dFpz7/3ve990nX8x3/8B9A82d3VBz/4wdztXLly5bQelZ6eHo444gjWrVs39VwQBFNJRZqmDAwMUC6XOeKII7jttttyryuPd77znVP/7TgOxx13HMYY3vGOd0w9397evlsbHceZSiqyLGNwcJAkSTjuuOOmtfE//uM/8DxvWi+Qbdt84APTZykaHBzkf/7nf3j961/P2NgY/f399Pf3MzAwwJlnnslDDz3Eli1b9um2i4iIiPwxUo/Fk5hMGCYTjL3ZWwKydOnSJ13Hxo0bsW17t9jly5fnbueiRYt2e66jo4OhocemMMuyjK9+9atcdtllrF+/njR9bAq2rq6u3Ot6Ku1pa2sjDEO6u7t3e35gYGDac5dffjmXXHIJDzzwAHEcTz2/6/7ZuHEjCxYsoFgsTnvv4/fZww8/jDGGj3/843vtkdmxYwcLFy7Mv3EiIiIishslFk+ira2NBQsWcNdddz1h3F133cXChQtpbW2d9nyhUNifzZuyt5mijHlsYu/Pfe5zfPzjH+ftb387n/70p+ns7MS2bf7qr/6KLMv2e3vytPGKK67gvPPO41WvehUXXHABvb29OI7DRRddxCOPPDLjdkxu14c+9CHOPPPMPcbMJIETERERkT1TYpHD2WefzTe/+U1+85vfTM3stKsbb7yRDRs28J73vOcpLX/x4sVkWcb69es57LDDpp5/+OGHn3Kb9+QHP/gBp5566tQ4hUnDw8O79STMlR/84AcsW7aMa665ZtrMWZ/85CenxS1evJgbbriBarU6rdfi8ftscvpfz/M4/fTT92PLRURERP64aYxFDhdccAGFQoH3vOc9u922Mzg4yHvf+16KxSIXXHDBU1r+5JX0yy67bNrzl1566VNr8F44jjOtdwDg6quvPqDGGEz2auzazt/97nfcfPPN0+LOPPNM4jjmm9/85tRzWZbxta9Nrxbb29vLi1/8Yr7xjW+wbdu23da3c+fOfdl8EZE/buM5H84MHsWcDzfnozXnAzODR5DvYZHvkST5Hxn5Hnn3T97ltXfnfxTI9xjN+RiZwYM05yPvgWbnf9TI98i9g7x8j7Yg/yP39uSjHoscDjvsMC6//HLe/OY3c9RRR/GOd7yDpUuXsmHDBr71rW/R39/Pd7/7XQ499NCntPw1a9bw2te+lq985SsMDAzwghe8gF/96lesXbsWYLeaF0/V2Wefzf/5P/+H888/nxe+8IXcfffdXHnllU9Y1O/pdvbZZ3PNNdfw6le/mrPOOov169fz9a9/nZUrVzI+Pj4V96pXvYrjjz+e//2//zcPP/wwK1as4Nprr2VwcBCYvs++9rWvcdJJJ3HUUUfxrne9i2XLltHX18fNN9/M5s2b92kdDxEREZE/Vkoscnrd617HihUruOiii6aSia6uLk499VT+9m//ltWrV89q+f/6r//K/Pnz+e53v8uPfvQjTj/9dL73ve9xxBFHEIbhPtmGv/3bv6VSqXDVVVfxve99j2OPPZaf/vSn/M3f/M0+Wf6+cN5557F9+3a+8Y1vcP3117Ny5UquuOIKrr76an75y19OxTmOw09/+lP+8i//kssvvxzbtnn1q1/NJz/5SU488cRp+2zlypX84Q9/4FOf+hTf+c53GBgYoLe3l+c+97l84hOfmIOtFBEREXn2sczj742RA8Ydd9zBc5/7XK644gre/OY3z3VznhF+/OMf8+pXv5rf/OY3nHjiiXPdnGe917zmA08eJCLPONdc87UnD9qD17zus/kCZ3JZM29sI2dcKWfcSCVn4AwWuuc5THaXJvlXXci5g/LO0ZI3Lu9+BPCfPATI/xmOP3nIlDTKGZi3kftDPWdczs+6bQZfsJF8Yddc87FccRpjcYCo1Wq7PfeVr3wF27anisfJdI/fZ2macumll9La2sqxxx47R60SERER+eOkW6EOEJ///Oe59dZbOfXUU3Fdl5///Of8/Oc/593vfjeHHHLIXDfvgPTBD36QWq3GCSecQKPR4JprruGmm27ic5/73NM2za+IiIiINCmxOEC88IUv5D//8z/59Kc/zfj4OIsWLeLCCy/kYx/L1/X0x+i0007jkksu4brrrqNer7N8+XIuvfRS/vzP/3yumyYiIiLyR0eJxQHijDPO4IwzzpjrZjyjvOlNb+JNb3rTXDdDRERERNAYCxERERER2QeUWIiIiIiIyKwpsRARERERkVnLPcZiX1V/FpEDi0rZiMg+k+6HZc7ZaNCZFGrIKc27g2awI/19XMdibB/HAXg54/KWc8hbD2RGK4/3/cqtnNfvc/87nPOzzlu6Yz9Qj4WIiIiIiMyaEgsREREREZk1JRYiIiIiIjJrSixERERERGTWlFiIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjM2pzVsxQREZE/UmmSP7aa81Ql7xlNI2dckDNuJsvMXbV5BtWd81arzivvfpxJlfXcbcxZ/drKW00bwMoZtx/KfpuZHET7cNUzOiZyH7y5qMdCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrCmxEBERERGRWVNiISIiIiIis6bEQkREREREZk2Vt0VERGQfyXLGzaDar8kZF+c8pclZ3HkmBZYhZyVxZz+cdjWinIF+vrC8xaJzLg6AWs44k/N690w+m7yxjbwbNIOq8bkV8oXlLTie95AAMKq8LSIiIiIiBxglFiIiIiIiMmtKLEREREREZNaUWIiIiIiIyKwpsRARERERkVlTYiEiIiIiIrOmxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1Vd4WERGRfSRv5e39cfoxkjMuZ4XldCbXXnOW807LOZc3k9LJefd5TnkLMc/kI8xdMTrnPs+7PMhfuT3JWXI8ncGGuznLfufdnryHpJUzDphhifknpR4LERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrCmxEBERERGRWVNiISIiIiIis6bEQkREREREZk2JhYiIiIiIzJoSCxERERERmTUlFiIiIiIiMmszKcguIiJ/tNpmEBvljKs9lYbIgczOeVphzeD0I80buK+PJ2cGseV9Gsa4P4N1m3xhVs7F5Y3LudoZxXo5Vx7MYN0DeQ+gLGdcPf+6k7yfY84Nyrsfc39nAAozCX5S6rEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrCmxEBERERGRWVPlbREReVJrWh/IHdt/WFeuuI23zqS6sDwjZDmrrvsz+OzzFgYe784ZmLO6szWDytsmZ0lkk7esdd4q0ECY8xpxvI9X3Z4zDqCUM25HzrjhGaw7yPk5NvJu+AyuyXv7uKJ2mDOuLWccwNZ928egHgsREREREZk1JRYiIiIiIjJrSixERERERGTWlFiIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNVXeFhF5Virnijr2mIdyxQ3ekX/N0a0DueJWrMlXUvmBW9vzr1zmWM7K0jO5rDmeNzBvaemcpbxNI++KgZzVnet5T7vylmIG6kM5A4s543JWi27NuTigs6s/V9xgW87q6ffnXzfJDGJzyX1AQpy3VHbO4yfv4TOaMw7yV7bPST0WIiIiIiIya0osRERERERk1pRYiIiIiIjIrCmxEBERERGRWVNiISIiIiIis6bEQkREREREZk2JhYiIiIiIzJoSCxERERERmTUlFiIiIiIiMmtKLEREREREZNbyFgcXIQxDWlpapv62sHBshyzLiNOYJEkYGxvDGDOHrRR5dluwJt/3q4WtueKsfGG4a8r5AoGD+sfzBW6t5QprXzM/97qHb63njpX9wcsXFs1gkXkvgWbODBaax0yWV8kXlpZyLm8mp2ctTx4C0J5zmTn39/KutfkCgSL5vuvzg+254u47fHXudXN/kjMw7+ed/7cw9zLzrjrvdyHOGQeQziA2ByUWkttxxx3H+eefj+u6GGMI3ZDuQjfjtXEe2vIQ6zeu54orrmB0dHSumyoiIiIiTzMlFvKkwjCkVCqx4KAFHHr4obiOS5ZlFP0i88vzGamO0Agb1NM6hVKRKI4JfB9jDOPj42RZNtebICIiIiL7mRILeVKnnXYa733vexlLxrh1061EcUS9VscPfVq7W4mSiP6xfka9cY4+8QUUvQIvOv54quPjXHbZZWzbtm2uN0FERERE9jMlFvKk5s+fzwte8ALu33w/9952L/WoTrVWxU1dRsIRUpNSp04UpLTP76Sr1Mnq1asZGxkhDMO5br6IiIiIPA2UWMiTGhkZ4ZFHHmGwMUixp0jgB5TLZbIoIx6MKZVLLD18KUMDw2zceRNRNsKGLeuoDlWJopmM0BMRERGRZyolFrJXruvieR7GGIaHh6k7dbySh1t0oRvisZhkJMEPfToO6iBxE+JiDWoJw2PD1MfqpOk+nm5ARERERA5ISixkr8449Qz+9DV/iuVZ9Pf3E3VEFA8ukqQJjf4GTuZQ7CriFTxqgzVM1XDwQQeTjWdsH9rO+MA4cTqTOc9ERERE5JlKBfIE27ZxXRfXdXEcZ+px2KGHcdaZZ7Hi8BVUahXiLMYv+biuCxWwYgu/6OP4DkktwcSGtrY2SuUSY40xRqojGAy2beO5Hp7nYVnWXG+uiIiIiOwH6rH4I2dZFi972cs47rjjqFQq1Go1IiJiYnoO7+FX639F3anDkeC2ugSlgDRKqdaqeAWP1kIrnu9RLBbxPI9DDjmESkuFR3c8ihVbHHv8sQQm4CUnvQTP9bj0ny5l7cP5i+qIiIiIyDODEos/IrY9vYPKYHBsh9VHreblL385g0ODjI6OUqNGfeJ/Dw0+RNAdUFhQwAkc/MDHtm3iOG7+7fv4gY/vN3syOjo6cC2XrJBhlS0WL11Md7GbV7/i1YR+yPd//H0efuRhmOi4UI0LEWD+QblDDxq7NV/gUL6wzYvyrbvv/gX5FgjQnS/syIPuzRV36Pb1uVd9a3lNvsDxnCXHZf+YyYSBjbyBeW/CyFe9npzVopvyVnfeD7rzncqVF4/lirNzlmJuq+dbHkAj9HPFBSbfhC9+MfdBQeQF+QLzHj6NvNXTyV2IPnf167yH2UxOrVR5W56KMAx561vfyuGHH05/fz+VSoXB8UGqUZXioiJr62sZs8aoFqr4JR+/4FMsFCmVSsSpoVYzNBp1qtUaaZrQubATP/Qploo4joNlWdi2TaFQwMbm0FWHUh+vM9wxTDWucv0d1xNkAccfezxHLjuShcsW4oUeV111Fffcc89c7x4RERERmSUlFn8kgiDgzDPP5NRTT2XdunUMDg2ycedGBscHoRu2RFuo2lUaYYOO1g6CjgC/zafQWWB0Z4PRTTUwMVhVPM+jpbsFz/MIw3Bq3ISFReAHOI7DQUsOolqtUjEV6qN1bn34VvyGz9HLj6antYejTziaQmuB3/72t1OJhWVZGJP3SpKIiIiIHEiUWDzLBUHAi170Ig5edDAjZoTfPfw7Rqoj1KwaySEJBa+AV/BwfZdWWklICMOQMAyJx2sMDg4SVYGaheO7+KVOPN+hUGgO9J7GAA2wsQmDEM/zWLFiBUkjYbRzlLSasnNkJ4PJIIN3DuLZHiuWr6Dz3E7mHTKPcluZ6667jj/84Q9zsatEREREZBaUWDzLBUHAiSeeyBErj2CH2cFdG++CEhBAcUGRsDOkSJGAiXsQLaZ6DerDw4xs6WsGU8BxWwjCNnzfIgxhtwmeMpqJhW0TlAJsz2b+/PkYY9jevp3KeIV1d61jfGCc9Q+sx67brF68mhesfgGrnreK3oN72bBhgxILERERkWcgJRbPMm0tbZz90rNpbW9lU2UTmZuRLkzpK/ZBe3Oshe/5OI6D53o4dQeHiZ4HD4xjqNfrzUctBkKCYolCWyuuH+AV4PEdFVNsmgPyDBBBFmeMJqMYy+C6LqVyiYOPPJhGrUG0IyKrZozEI4yn44w9PEZpY4nOnk7OPvts7rvvPtatW/d07DIRERER2QeUWDzLtLW28ZZz38LBiw/mph03MRAPUClV6Av76F3YS7G1SIkSHh5Ugfoub7YAB2q1GsPDwxOvhQSlMh0LOnYJ2kNvxeRLAc0ZBiqQZinjtXGMZSiXy83B3vOKGNtQ2VGhMd5gw4YNDA8O88jDj8AgLO5ZzFnLzqJeryuxEBEREXkGUWLxLNHR0cEZZ5zBvAXz2Gw20z/YTyWsYJdsOjo6sAMb3/KhATExmcmmphhzXRfXcZnsuEjqCfXhOoEfELaFuIXmYWJlFuQppJ0BZmIwN0GzSF5kY6UWuGBsA1lzsHZ7ezuBH2DKBupQG67x6PijlA8qc/Tzjubg7oNZ2LWQO++9k1tuv2W/7DsRERERmT0lFs8S3d3dvPvd76a1p5Uf3/1jhncMM+/QeRRaCvTM6yEIAiqVCkktIWL6PNFFv4jrP3YoJLWE2mANf15zVig/mJh/OmFG03pPJhaTg7one0SwgbSZWHR2dkIXUATjGh787YNs2bmF1sWtPH/e8zl51cmccOQJfPNfv6nEQkREROQApsTiWSJOYzYPb6Y9bMdr9SibMmExxPd90jQliqInLEZnMKSNBmkUkaQN8MEJHIIgwLVdiCBNUpJdqrPY2Li4WBO3R2VkxE/UpWHAizws28LLPGxsarUaSZLgJi62b9PT3UOwMiAaiohHYzY3NnPjAzfitXq85uzX8MiGR7jznjv32X4TERERkX1DicWzRC2ucd+2++h0Ogl7Q4phkXK5jOM4xHFMHD/5PUxRpUJjZIQ4rkERvKJHsVhs9jZUISGhSnUq3sPD3eUQysioUcPspaqphYXTaA4WDwgwxjA2Oka1WqVYKhL4AYcsPAR/hc+WzVvo29HH/ffezw233cBJh53EBX9+AT/4yQ+46967VO9CnhmOyFlR289/PA/4HU8eBIy1tOZb3l05y2THM6hU/Wi+sPvNqlxxayo5q40DtOeMG8+/SJmJnMeyv6eBentRffKQGa07d1nimVTTzns6te9Pu6yD8m3PIjbmiqvnLYs+g90zRjlXXMPKt+6olrOaNsC8nHF5j7OZVKre22Q3jxfnPHbjnN+bmZwi7eNDUonFM1xrayvPec5z6FzQiVNwiIkp+AUcv1kN29rjKOs9SxNoNAxpyuQYbaCZMCQT/9tVRkZEhG3ZuK6LZVn4+HtNLDCQJAmZyaYSEtd18X2fODNEUYQVO1ixRVgI6erqIjk4wSSGWlDjzk134ra6vOxlL2Pz5s3cfffdSjBEREREDhBKLJ7h5s2bx7ve9S4KnQXurd1L3arTVmjDD/0ZLyuKoVrdfXz243sqJqWkVKniOi6lYgnbsilQ2OvyjTFUxiukaUqJEq7lNit32zb91SqVWg1TMBgnpVwu09XVRWtLK92Lutl09yZ+9Icf8bwjnsff/u3f8rOf/Yz77ruPJJnJFSURERER2V+UWDzDZSajElUgBbvVxvM87MzG2lN3mUtz4PRepKRExKSP6+d7fA+EZVm47mOHjg0k1SrGmIkeiWbvR5YZknqCZVmU2wJs1ybLUgyGhISMjMzNsAw4drN5Jk2IIgsHByd1mgPAw4ByV5muxV2kfsr6LevZObRTvRUiIiIiBxAlFs9wjbjBpoFNtBXa8JZ62L6NXbN53MRPzVubSjxhYhERUaHypOt0XZdisTh1m1VSr1PZsYMkiqhUKsSJoTIOcZRS6avg2BbLjuql0BpCqQSuS32ygEYA+BDUmk3MGhEVIkzVNBOcEEqlEgcdehDt89sZeHCAn//q5zzywCPNKXNFRERE5ICgxOIZqq2tjZUrV9I9vxvLt4hNTGAFOI7THCz0+A4Law/PPY5N84BwbBfb9vBsb49xaRxTGxsjSw1RlJA2YuLhBmmc0qjbpMbC8gIcDLZtQZYxPhSRJBktxRDXdUkbYDKmBjZlaUoSxzixg23bGM+AC5axIAHf87FbbcbKY8SlmNb5raxatYqR4RE2b96s3gsRERGROabE4hnq0EMP5TOf+QzGN/z2od/SoEEbbXiWB0V2nxEgxxhuHygDoV8mCNoo+HseL9GoVBgaGKBaidi5s4KdORRNAct4QBE7DCgu6QVjY0bHSap1tq3fiOPHrFrQQrEQUB2HpE6zrS7UG3UqlQoFUyBIAuig2cPSACpQbC0StoeMj47DUjh80eG87Pkv487b7+Sqq67KNeuViIiIiOw/SiyeoQyG1ErBBbtsT4xfyEjimDSOMZmB1AZjkZLufaYmIK2lBF7A+ECF8aEase/SCCySLCFzMuI4pl6vY4whTVPSRkRchzT1cIN2XMuj6LfhWB6OU8IOAgrtnZgMsg6XyK8SDblYTkpmIEvBGKYnP6Y5uDs1kJiJyQCtXV5LDFmU4TouLe0t+DWfdDwlyz1toIiIiIjsT0osnqEq9Qr3bbyPsDvEX+jjOi61uIapp1R27CBtJFApYVKbCpXdBmTvqmgV8fEZHBhgZHgYrJ1YWLTNa6NzYSfGGAzN6WBr1RquW6AQduIVWulZsohCoURvTy++71MslbBtB8uxSeKYvvIOapURWLeNJBkjSh0qVTB7aU6D5vCQx89pFVdjklpCYAUsW7qM6qYq69etp2+kT2MtRERERA4ASiwOEB1dHXR2dRKbjCjLsJk+zjozhizNsLHwsOns6mRgdIDQDUmKSbPnomFjkoy0bpFFLjYBlu3ieS6OndHsB9i958IlxDEeQTmjiE+WxGRpjMGhXouxLBvbtrHw8Qsugd9CS0svbqFMoa2VICzglUIc18PxbCzbAtvGMjaObWFbkKWGLMkw2cTYir10oNjsZTjIRI+GYzsEYUDkR+CCcTS2QkRERORAoMTiAHHii0/krNeeRX8jYlujhg/TKkLEcUy1WiM0Dt0UGY/GuGfdPVhbLbqr3fieT5EitnExWSeW7VHs7sUNixR6unACn2ZZyd3rPlhYWEBvFpAZj8ZwH/WRPsYr44yMjhIEHmFYoKWznY6DDqJc7qS7ezG2Y7DsBqllUbdtsiymUhsFy4JikcykZNVxzFiF2s5x4qhKuvSJS1ZOTBLFnoeNg+d5uAUX026ozasRbg5nVARQ5Om0ZiRfxehbtx+Te5kbVi7LF3hf3krZM6iova9tyrfu7NgZfMdvU0ntuZWzB3ksb0linnA2w6ey6vzlkFvyLpD8pY5zxnXkP+Z7vb5ccd5uVar2LNrtnoG9qOULA/DLj5+qcs8G6cq3wJkMq9x7ea3pxmawzLzyHhb7/Dwm3/4GIJh53bMnosTiAOEGPsXWFlpsQ2RneAbC7LEr+HGSMF6r4acWpcghHgcz7GJsC9sJcdyAwG3DsX1su4Tt+BTaOvEKIYXWVhzfp/lxP1FBuRDw8e0E3zVYfoHM9vF9nzAMKbW2UW5po1As4wUhlpUCMQaDRYYxKSZLwLIha/ZO2I6D7TiYzCZLII0T0jjCtlwsx36sa8IGnOZ3a3Kbd5WRNceJGCBr1u9IsoRCscDSZUsZGxmjr69Ps0OJiIiIzBElFgeIofFxHu7bTumIg5h39FKsGKxa8+p9CMQGyiajMTZO/6Yt1CsluuxVeIHHvEPnUSiV6e1egu8XKBXLUyf0lg2WE00kAcUnaUXzJiTT006WHUGWZWRpimVZWLaFbWc4Xkya2lQqVZqXDCqYyaHhxkw8gCpYxqHY2Y3th9heK6YaUxscwUrHKXR24RUKj12JCnjCOhsxcbP2RQykMDQ8RF9fH0sWL+F1n30dd9x+B1/60peo1WZwCUVERERE9hklFnOso62D1pZWWtpaScgwroVbDLAaFmTgWOBZzSv5KWDIsNuK+J5Le+LhBR4tbZ0ExSJhayu+XyAIi9i2jW2DZU1Ov2Szaz/x5LNTszPteqHf9nEI9tDaZt9jmkKWTY7Z2HVpuyw9y7CwcHwPNwhwggDL90mTlLhhCI3B2AZjDFmWkZp02uxVxhjSLG1udAoJKfHk+lKI4oRa3ADXoqO7g5a2Ft0SJSIiIjKHlFjMsdec/RrOPftctgVDbAuGCcIAxmlOj1SjOdggbH5QJaDQ0kLr6tU4xiFIAmwLXD/C2BC7HpnJqNar2ECxCI5jNReATXOMxWNjHCZnYHrsPyZF7PkGxsmT/sm/J1uVTDR28oUMqGLbLoVCN67jUF7Ug2mziNIB0kpCMW1uWr1eJ5qo2F2pVAjCgNAPaUQNovEI6s1F1zHTbuccbiT0UcGv9HH3+nvYsH1DMxERERERkTmhxGKOdXR0sGTpEuqRzY5oHMt2muf0Cc0cYGKMmTXxn47j4BWKOJZHkQI2GVAhxZBaFlnW7NUwxpBkhgyLLIsxxgYTgYmbtzZZFqnjYGxn4jYmdul0eHwPxBPZS0lvy4BlsCyDZRssz8bybNLEYLKMOE6xo4R6PSKKIur1iEYjJnZTEjdr7gMbiJqFLSZ3yaQ4TYiIaWQRtahGI27MZLeLiIiIyD6mxGKODUQDPFx5mEEvgnIZLB8q7P28PgEqkBJRZWwiMMW2bQrFYnOq1yKkmWG4UaORxAwODNNoNKBWwUoTwjDE9316FyygtbWNug+xxx56Lp5MSrOn4nHzx9o2FApklkW1MUijUmV8x3bGRwYhGAUnobbNwrI9KpUKURQxPj5Go9Eg7Qyplu1m54oFmAZkjckx21Pq9QFgHNftolwqEYbhTBouIiIiIvuYEos5FmcRtbhC5EBqrOZMSpNn0NZeZiBLm8lESgQYLMtuzqZkwLYMcRqTJAnjtSq1KGJobJh6rQbVKlaSUCgU8H2ftvZ2TLGI5bhYto2xZjqjkmGPs0xZFjjNnpAkjogbNaJqlahawXJisDKSegNIqFRqRFFMrRYTxymNOMFPmrdhNW+5itg125ncH1kW0UxssomB5bbGWIiIiIjMISUWc8xvQKkC20fG6UvGWbDYZd787qnX7d1mSUppdmkAGGzbo1DoxLYtbDuiXq9y9913MzI2Rt/4OPUkIY5jsuaI64nbkGJMllFpNDhk/ny65y2ita2HBg3iGU0O/cRMklEbqDI+MEr/hkcZGe4nPKQVt+RTKNSwbYt6vUK1Gk0MBodmt4lNowFxPLm9j/E8CHYZV54mKZVKhXq9rqlmRUREROaQEos5ljRiGiNV6mmNRlIni2Jc94lqpTR7KybHNjSrYrtYVrPWRa1ep39ggKGREfqrVaK0eWI+edJtjKFarZImCaPj41QqFTrjDBeH2JqoK7HL+flj7di9QQYLsCdKae/6QnPaWWMMaWxI45SoVieq1XAaJfAzTGDAhjSJSZMEJsZ9QIYxCVkGyR46Q5ypukYTU+NO7BIlFSIiIiJzS4nFHPufG2/kvgcfZPFzVrP46NX4TzrGwQFCbNuiUIA0TRkY2ES1Xmfd1k2MVytsGxggimNc38eb6PIwxlCv10mShCAIsIKAcqlEqVTC8yZ6QfyseURM3H30WO+AB3uYfjYhpg6QJrBr/Ygsg1oNG4viwg5o8eievxjXDhmrDBDVRwn9dhzXg0YdKnWCtja8QoE0dahUmot4Ys0ZqcqlLpYsXsLozlHs3bt3RERERORposRijvX19zM4PEzLQQexPLNxzN4GVkyyAHfi1qeMJImpVEYYGR9nS992KvU6lUYDYwylMMRxHMxE78HkbFCB7+M4DkEQ4PsBtm0BcXMWJhtImuu3bXBcM1Hrwmp2ROxyxm/Z1kQXgmm2eddegyTBsm3cUohPkVJLO3GlzujoDpJGjSwpY1IX0gwrTXFtG8/zJt+6V4/1TFiAh217+F6A6+hQlgPPrb3PzRe4cwZJ8X1PrS3PZLffNn8G0aP7rR2yD6UzKWbq54xznjwE2OPYwD2ayWyDpZxxQ7miysvy/5u2wGzLFfeQdViuuMNuX5srbvi5rbniAFrNWK645eahXHG319bkXjdPeqFywkjOuLyH2Ux4+3h5bt7vDJD/Y8y36n27OJmp1551Fme95CUMlX2GSz7+vA4olZoDDKLduy8cp9mLEEV1Nm3qY3R0hAcffJBqo8FImpIBQRBMFMizMcYwPDxMHMeUXZcW1+Wwww6ju6uLed0H09bSjvGb9bPTKII4xnPK+KUS9bTCjkqF2uggY/0VaqNjjO7cOXWnVFtXG4cevQzHc5tFM7IM6vXHEoyJngs3Tjho6RJaWtsYum0r9dFxhrcM44QulhtS6ilTbA3wQ2g0njixSJKESqVB2sgomSLr71nPJddewtDOQaLGjKa0EhEREZF9SInFHLKAFcuX8/LTT+fW0T5uHd2BUwqb9yA1C1JMD6bZMeB50GjEDI8MMzgwwNatW2kkCZTL2K5LEAQ4E4MR0jSl0WgQNRq0lcsUHIcFXV0sXHgwxWI3vl+mSpWGqTdLascxtm1wXIckzqhUa4wOjTKwtZ+xgUF2btrU7DWwYP4h81l8+AJsu4hVKDx2/5KZ/H8G4hgrzWjp6MC2XVzLhwjqYw2sRkSpt4xXLuEFze2Kn2TseJqmJEmESSwCK2D7zh3c8+tbyBIVxxMRERGZS0os5pAB1u3cyS8feIDtpZRqCYpWApUKRFmzloPrgB80u94CSI2hWqsxODLCg48+ynilQuz7WL5PWCxi2XazZkWWNS//G0NPZyd+GLJ8wQI6W1qYP38+xVKR1EmpUGF4YAeVsWH8MMQLArbv3MLY2EOw9hHMPfdhdo5SfLSfyCtgFdpIAotGKwyH8OjOR2lr6+KQYjcWGQ0qzcSjPjGugzrGAn9eG7SGlNp7qIzVGHVGiK2EtraAYkeRtNqgWk332lvhec3H4GCNbdt2clB5PscuWsGD/Yb7rN/n7ukUERERkf1DicUcmbxVaWB8nLXbt1NbENJoK5CYBKIIM1WszgHfx7IBH0ySENfrVGs1dgwNUWs0MI7TrMjtN++pq9frZEkC1SqOZdFy0EG0tLdz8MEH09XaOjFg26NCRmQaVCujjA0MUOjqIgwCBkaH2b51K+F991K88RaCHaMUNg/gdc7HWnwktNikjk29Ms7Q2CC27+MaB8tyaGBhMgMxmMwQE2Mci6C7AKGHXyjj+2VSUyEmwQ5d/JJPtRYTRXvvdbBtg+9DksQMDVU4KLA5uL2H/lIb1p4qf4uIiIjI00qJxRywLIs3vOENnHLKKfRHFfprA4zdPsToL4fIVi0jPO5IXL+MH3TikuA3uy4AH8/xaC10kbVZHHLwIQwPD7F53TrqaUqjXsd2HGzbxnNdDl68mHKhwKJDD6Xc0kJnayvhxMBtDIxvH2B0uMLo0CDVsRqRW2Xc86j97gH4xf9j4ViNlZUijuPidhUZ6erk4O4SpeWL6TnthYxZDdaP7QDTYEvrFjzPoWA3q3A3aJCSNm+HMkC9jpOmzDu4B99NGXlgB/WhKlH/EFUrwTY2xaJNFE0fYzHZU1Fp1NjZX8FKPY7oOpza1grfu+n77Ny+gzTVbVAiIiIic02JxRywbZuTTjqJd7373Vzzy5/xi9/9muF1Gxm6dz0lMroWdRO0gQk6gRR/8gQdH9dyKfslGsWU7u5uTJayoVqlEUWQZdieR7lcxvF9ent76WxrY8nChZTLLRNrf6yeRW1wjNFtA1RrVaJGRNTawDQaNNZuhv+6k662No7s6sKyA2htYay1THtLQO8hC1l10mlsHNjOjt/9D40sZmB0gDD0CcplICMiIpu4QckyQBRhGUN7dyu2FePfZ8FYg3i0QhRCsVjE932SZHrvg+M0eyqGahH9Y2P0mG4OajmIh9bew2//+ze7FNYTERERkbmkxGIOGGPYMTTEQ5s2MTQ4hFWrcVgtobsGja3D1O56iOpB4wxZNi3FgB6rTFarEQ0Ps6Oe8MBAA8u16G5vIcRQOfRQoqhZeMJxPbrKnRSCkPk9vRRLRTLPowbNW6vS5n8Yk5CmNbBq0G6DW4D1D2HduIOlD21hfqGT+a3t0NmJQ3Oyv4plqG0ZJFo0AtUqnW6Rk5cdx/b6EDdvfIBGMaDnsMN2qydhjKHRaGAB5a4uvEKBroOXEZuA0b5+hrYP0X5wL8X2Fny/RKEQAjEQU4lqDFRr2A2LhXQytnGYW+57hMHt/WQqiiciIiJywFBiMUcGRkfZ2NfH6MgI1Ossricc24C1O0a5a+1GGmlKtbtMnLXQUnBJ4phKpcLISJX163fQ0dHKCQcdTUvgUT3kEOKJ6ZRc22V+eT6BH1BuL+MUPFLHITNABCaG5gCIBmlaBxrQWsS0+Fg3Pgr/+XsWDoUcG7ZjldqgrQ3XsgiMwR4fp7FjJ/HgGKZWpy0ocewhq3hox3r++74bcMoh2dKl2P7u8ydHUYRlWZTa2ghbWmmbdwiVus3APVsZ6d9B5haILZ/u7haCIKA58XTMQLVO38gI80wbC2hjYOsW7rrx99NnzBIRERGROafEYo5MFqtrxWMBBTooUCCkpWLRsbXOYEudgaEadULqZYNlOYRhSFRLCAEvaxatdr2ARYsWY0zzlqC4EbP94e0kUUJ5sExQCjhkxaEUWryJDgtDHEckaR27q4VCW0Dl0W3UH9zMwZsj5tc7WLR6FZ3LVlEf7qc2sJ2k0aBeqxFP1NWIspThep3A9SiXDHbBJiyE4LpEUUQ6UZDv8YwxRFGEyQy9vb0U3AKtocP46DDDlSFqW2uMVgZIiuPNgi1l8BKPBXQwvnGYWx9az85NO5VUiIiIiByAlFjMsVZcDCHthIQUKI9DR1qn0lGjPlSn5kTUOiAIbEqlkMiPKFgW7sRssp4fcMghXThOc2zC6PAod910F8MDw5TLZUrlEgsXLSRoaSGheSdUFEXEcQO7p4Ww5JHd9Qj129bRtSViZb2DBavW0P7asxi55RZqv/wlaRSR1utTvSJR2kwsSmFAKQQ7tAnDkNS1aDQauE9wi1IURZBBb28vvR29zJ8/n3q9zh9+9SuGNj6MPVInCoCDC+CEdCQhPZS5c+NGbvvv2/b/ByKyL901mDMwbyVgkQPdkxQjmuQU8i8yreQMzFsSOW+l+xm0McgZ18g32UgL1dyr3mn15IqrZ/m2Z91zD80VNzrSnisO4LC2B3PFtQ6O54prmzece90jD7bnjMxZDT6dybGbMy5v4fZ6zriZ1AueQZHuPJRYzAEDjNZG6Rvuw4wMEIyMUO5up/35vVRoMGrVqHSEhCPDhO1lwraAwLMJfbA6HBYd5uJ4zdmdsixr1q0wGZWhCiPDIwyNDzEeV2gr9eC3tpJkLo2GIctiIMUrFbCNx0itRm2sglm/jfDBjbQ0yrS1FQnbQmgPob0M7e0k4+Mkw8PESdKsYFePod6cUhYDPi69tDIexwzsHMANmwPHHcchjuPdei8MzWloMysjbA8pWAWOPH4N85cuZcMDd7Fz80ZaQo+iMezctJ2NW0bYtm7bXHxUIiIiIpKTEos5YRiuDLNlcDOdAztoGxykdelKOpetJK6PENcGGG2MURzqp3BQN4WOAoFjUwAKhVbauxaRJCmVSoUsM9TrdZJGxPYN2xkeGqZ/tJ9GErO4XCJs7yBOXep1AzTASvBLJYzr0rdxlKGdg7B2E4W7H6b14MPo7O2m0FmAzgJ0tkBnJ8m2bdSGhohMc24qalEzsW80t8bHZQEd7IxHeXR7H165wEEHHUQQBCRJssfEokGD1Erp7eqlWCzRdfAistRh6F8GefCuByhYBcK6Yd1tm7n/tvuf3o9HRERERGZMicVcMOAmEDTAjNZp7BghWZRAWwCpBSMRRA1oNHCyjCAIsE1Go1YDkwAZWZZhTGNixiXIkhSv5FG2yiw/bDlJltHd3U2pVCbLGtTrGZBhWc0JoCwDtR2DjK7fzPxqSmfQSovjY1nA0Cg8shlnxwDBRPXuLAioWBaR65KUC5jAIjIR/f397BjcyeaRnYwTUegu4hcLJEmCZVl7HGthYeHh4eCQxilR1MB1QyzbY9Hhy4jjF8BQDWuswaqVR7Ny6UoefPBB7rnnnqf7kxIRERGRnJRYzBE/gkIV2FmhtmmA5DkJdBWhYkG1Co1qs3J2llIsFEgadSq1GhgDPHYPYpZBfeKeu6AtIGgLWLNgDcZYWFYZcMiyCrVaCpTA8ii1gOMaxjduZ+D2tawYjTmi2EWHO3Hf4LZ+uP1BvL4+CtUqdpZBocCw71MtFim1t0AR6qbOwJYBtg7uYG3/JkzBZd6hy/CLYXMQ9xMUrgsIwEDaSKmaGsViCd8POfK4o1m6ejEP3XAXm/6wlheeuIYXHHMMV1xxhRILERERkQOYEounkW3bHH/88SxbtozlK1Zg+T5OsYhTLkO1QbRxG86OIcpjDdoci65ymdBxGa+MY5IMjEdzGtY9DfS0sCwPsPC85t9JYrOncdQJCSkJaa0BYw2SakTUiKilKWPAaH8f5sGUuDJONDZGNasz1uKz1XHZEMBAfZixh+/A9hwsy2Y8rhH2tmKHAUGhBc/3sSy7OZhkchyfC1jNplupBdPq2hmSpJkd2Y5LELbResh8uuoxg1aNW+6/k007tu6bD0FERERE9gslFk8jx3F485vfzOvf8AbuXLuW9Vu24Le3E/T0wEiV6h/ux6nX6alWGe32WbSgh8jzGOjvx7MDihSxSNhbYgEFbNsmDMGyoFKBNN09s2jQIDMp6VgF+itEo1Wq1SrDE+Mhhjc+zNDaP0AQQFhgFMNAd5GtwP2AGd2M9dutFEpFent7KLS20nPoUvywRKnci21bQGWiO4VmglGiORFHfc/Nj6IxomicYrGLMOygd7WFv3we9//3b7n++v9g24Mb9tGnICIiIiL7gxKLp1k9jhmv1xmpjTNUGSLOGhR9i3EixhtjZI2YNIoYjS2qcUyaphgDZqJg3OMu9e+i2T1gjE2SNBOLZm+FmXiPoXlGb3ASF9eGclcr7Uvn0XAc+lpCGj3tjHaWqeBQoYhdLOK0lKmkGfU0xcGmGwdTADohDEPa2roIigWCMMTzXSyrObYCDNjgFtzmLU92SkZGQoLB4Lpus0K3PX0bkqSBZdm4vkOL30KxtZWwtZWFSxbR45bZuXMnmzZt2l8fj4iIiIg8RUosnkbGGHaMjrKur4/1OzaxYecGOpMR2krQQoWWSpV6Heo1w+Ziws6qwWs0KGBoTob8RPNaG6CGMVDb61TMdTAWYb2Im7rMP+xgSu0BQ48OsL1/lGKxSBiGmPYQ0x4QtrVT6OygHkVUq1UK+KykCL4FJfAsj7JVwiLDsqsTbXhskmXbsSm0FrCwqFQrpHFKnTqplVIKStje7nOJT/ZcFDo7aG3toXfhQvqXLuWQw1tYFrTz61//ms2bN+9xULiIiIiIzB0lFk8zkyRkUURmG7LQIe4pE/kOw1nG1jSl3oBaFUa7CjhdLbjlMh5W7rI/AK7b/FibvR2PPwE3U4Oq/WKBclcnWebitbTgBgGO50HZh7KPXyjheAV8ywVcfMul6ATNoyYE27jYqYttZTiOR5alRFFElmYkjQQLi0alAZlhfGSEJEmIswxsmzAo4/kuzYRpD22MY5JanUIhoOegHrw6jFZrdMzr5oUvfCF9fX088sgjSjBEREREDhBKLJ5mVqMB1SpWwYZ5ZaJFXYz7AevqdTbU6zTqUK1BubWF9u4ugraQom1h5Vy+bdsUCgUsy6JSqexxZqZ6vY5lWRQ7Omjv7aVniQFjqAGRZTWHa1gWNCyshoVxDbjg+1AsWM0paa1mrbxqpbnOYrFEHDcYGRkhqkVUdlSIGzHVapU4iqj09ZElCcWeHoJymXJ5HmGhRLMXZvdKrdH4ONH4OB2dJdpecBSb73mIe269n1WrlvOnr3kd119/PV/60pdIElUslqdfy5pyrrixO1rzLTDNX2WX3mK+uB07ci7wwP8Oda7JXxp28NaZlJyVfc4K88XN5GqZyVmWOO91JjOSM3AGp0iNnG0MunOFDeQusQzLeSRX3Pws32/CHTwnV5zf1sgVB/BQ9YhccUu61uWKa2c497pHaM8XmLca/EyO3bwnb3l3Zd5K3jOxtzvsnyIlFnPAMgbP8Qn9AsWWEsVSkawekdUaZAmECQRhAa8lxAk8Js7jAciyyRoWzV9Qy7JwHGdiXEPz9epYtTmuYTKpsHncWIbmbVnGGNIsAxywbKzJUAtsm+bRkUKG1RylYRKiakyaGeIsJYkN9UqKZRnq4ylxHDE6OEocxUS1GmmcksQxWZLiuB624+L7Ab7nY1uTt3ft5V+Cie1zXRfbt3ELAVbRg8DDuC7G3v02KhERERGZO0os5khL0MI836NzXidt89qo1wz1usH4QAhWHGNN9CzsKk1TqtXqVGLhOA6lUmkqLm7EbNuwjSzJ6O7pJgiCx2Zkepz61PILQDB1iu+FEAaAB7gQ0Sy0XRuqM7hlgGojor9aJYtjqFRJ4phqrYLJDFmWYduGYsFg2zbGFLFdl86DD8Z1XcotLXi+h+9nQIUnu8Tk+z5BEFDu7aB06DwatsuGnTvpHxvTbVAiIiIiBxAlFk+zNE1J0oRKrcJIPE6hq0QJsFwHP7DIPINxsmYFbmfXbGCiP8EkmDieuKBvY4wDxgFjQZpBCiZLwBgcy8axnV16ziZnh7KZ7APJsmbVbpPWm70hxpA2MmI3A5NBltFg4oal4Qr1gSFqSUItiiBJsKMIk6W4NuDYOF6A49qUSzaWZRHXbbBs/GKA53n4RR/P87CdiZmjnsRkD43l2HiBDynUoypxotsdRERERA4kSiyeRsYYGo0GY2NjPPjIg9y9fSOr7DWknUVag5C2ckAjiqmP72laJwcoQpJBpQqZ1fw7sKGl2Py7WsWOoRgCWJTLRbygSNWCFEMzPUiAkGZ3BIChMTpMND5OvV6n0WjQaNRpNOrQaEC9ToNmj4WpNzDjFYzrkoYhnudRLBQoFHy6utrwCyHFnm4836dULpHFCTs3biSOYqxyCdvzKJfKuK6L5eS78TCKIuI4xhhDuVzGGoupjPfRqI/kvp1WRERERPY/JRZPs/7+fjZu2EhjrEIBh7RSZ6x/CKtQwApDsixrzuaEISObKkNh2+C6BgsbLwgwmUWW+diO99jCLRtsB8t1AEM9joiNTY2MhIwsqTR7M0gBt9ldYQz14WZiESUJcZJQr9ep1Wo4aYqbZGQWGLs5fawThti+h91SwvV8CoUSQeBQKIc4voeNjckgqUVkSQJZhmUZXM9tvu7a2G7O8RGm+TCZwbEcwjDErqTN7UoT8o/UExEREZH9TYnF0yhNU37+85/zq1/9itNeegYvPe5Ytu4Y4uFNv8UpFHAKBbq6upg3bx4RETVqzQmTalAsFOnsgiB0aVuyhDSzqFZtIMNy6oAFpQKkPlAmjho8tHUbUZxiqJJlCbWBcZJ60ow1QL0OUUQ2MS1t0NaGXy5Tq9Wo1+u0lMuUuroo+mAKUHKLdIYdeCWfYncRy/axrBJZljV7OWpV+jdvIa7WqA0OgMkI2j28QkD3vHn4hWZl8BmJgAYU3SLhvJDB6la2V7Yw2mgorxARERE5gCixeJqNjo4yNjbG6OAQteExPMfQ5RaoRIZKY5xRY+NkkHmGzEshaU77SgKhO46f+DjFMkkGjXqGMZOF8yygOR6hXm/ePlSr14miBJNVyLKYerVO2khxHAfbtsmiCKII23FwHQc/8AiKAThgu3ZzxqpyGeODCaEYlCgVW/BCj7AQYjKbJIY0zWjUIxq1iEa1QVyPSOKk2YHi+s1xF46D4zzJHG1ml8ekrPlwLAfHdbBdh8yxyB4bJiIiIiIiBwAlFnPAGMOvf/1rbrvtNv78ve/hz1//Fn51+2388vbb2bjuPm7Yuo7eeb0sXroYy/MhKGCnNpvjzXgFj3JvmTRLGa/UMNnEmTcAdvPOqSzBssD3Q8CiVhsnTVNCL8Rtcens7KRQKFCrVIjjmI72dkrlMsXOIkF7AZM1p6K1ExsndogsqNsQ+DblkktaT6jsqFAbG2dg81ZqjQY7KxUM4LsufuDSecQh+GFIubUH1/Vw/Jy3LjVo9lJM7azpL3tFj9K8EkFbMPsPQkRERET2GSUWc2R0dJSx0VHGR0ahEeNbDq1hARdDrTJOo1YmbUSQGoyxmj0XdQsv9bGLNpnJiGo1DOaxK/cGLCws28JybHzPw7YdsjQmTVMKxUJzwHW5SKFQwHZtkiSh3NFGS0sLQVuAX36sEJWpGUzVkNE8UCyrWbU7iiOqY1Vqo+NUh4epxzFRFGG7Dm4hwAt9wpYifhjiF4rNOhvUaA4h33NyYU0U5kvTlCzKsKxmjT7LtrEsG2ya2zXxKBQK9PbMo1atMjKat9iRiIiIiOwvSizmkAEuv+oq/uO//ouXnP1Szjz7pRz0aC+lZS14nkcYhsS1hPpIBRwXSgWcokOpXMLzvGaNionJoiYnfbJtm2JXETfwKJd6cByPLKtgSLBtGwuLOInJsowuvwvXdXHc5m1KURIxPj4+1b5oNKI+VJ8cP06SxERRnUqlwsCOAawkwW008AsFlixZQlgu0LOoB9u1ibNmNW3brmNhUyDAxqJGjXQPpSODIMBzPbbv2M7Q9iHCsFnpO2htxS+3EIQBQSFgdMtOKn0Vjl52NK/6+Ev5/W238E//+k/E8e7Vu0VERETk6aPEYo5t2baNLdu2cexJz6OjvZXOSgedYz3YloXt2NjUiUYijMVkgWwsY7AtC9/3sT0bK2SX2aNsSi1FvMCnUCzhOB7NLo3m7VEYQ7VqiJME13fxPA9jMtI0IY4iGlGEbTV7BRqN+kQRvWZboyiiWq1Rq9aoNWq4BnzPxQt9SuUihZYS5dYy2BbVapXMTNTNMBnWZEfFrmXEd2FhYVkWSdyclcpxbVzfwVjZrmU3MAbSxFDuaOHQgw9l05ZHsS1V4Zan19hDrfkCD825wLVPMv5oVzuyJ48BHrtF8plvabIxd+wga3JGbn1qjZF9YyaTb2RJzsC8pzR5V76nqd/3Juf3rbMlV1iZ8ScPmjBCvt+jQlbPFXcMd+Zb8b35wgDuWPWcXHFpzs9wnHL+lS/JGfdgzrjdr43uXd6f4bwFf62cg0tnclo0NoPYHJRYHCDGh8bZ/sh26tiU5x1Ke3uR3t42+rf2s451mCRpzuBUrbJzbAw3CCh1d+M6NgVn4rw7Bcd1sKwKluuwM+onyywgxbIMhQI4TnOa2iyzGYqHSJKExsgIUaVC5nlkrktra0C57NOoGWqVFM+DIIDJH+NSe4nWha0UwpDujg6wLFLAMha1wRpYNpkfNufIBVKTMlAbxpgEP/Rx3MedRBlojDaaPSbD41QqFcrdPZTndWKnLta4RaVaYdgeZixJYd48xlybDePr6av3NaflFREREZE5pcTiAJHGKVEtIvMtnKBAUGqlrbubei0lbG3BNBpg0exVqFUxGKKogXEcPJq9GBjIXJuo6mC5NtVKlXTyQo8FJrZwXQvLKgIetWqVRiOiNjhCNDYGE7U0AscQ+pDGkGUWBrAcsJ1mLQsv8Ci2FykWi7T1dJGmKdVqFRMb0moKlgHXxmCTpilpmlKpjpOZmBanBYM3NTPVRLNJGgmmYUiimDRNsBwHJyhgqilJIyG2YyInIsFgFUISA+NplUbamLPPTEREREQeo8TiAFEqlejp6WFobCeVHdspFGwq9FDs6WD181c1i9mlKfVanf6BAUhTnCzD0JxEKU0y6kM1TJpRHahiWxZeCvbULUgWdUJs3yUMGzhOxGR/XtDSglcoEBQKeGGIV3KgZFMseLSmAXYATrHZhHI2USjPc7Cxqe6sYozBpBODyAuAyaBeo1FvsG7dOiqVMfp3biAzMQcfdhgt7e0sWbKEYrFEvQ5JbBgd7aM2Mkqa1iiVwPMCoMTQ0DYGt26ntbeVzoWdpGlGvV4liMHK16srIiIiIk8DJRYHmCxJSGp10jgmMeCHAa1hwORtSLVajdijWYOiUiE1hhTIohQzFpMmGWnSnHjWw+wynMFu9j6kFoasOVZjopq27QRAQFgoNAeEh4AHHiEhIVZgNROGx0toFtybTF7c5sNkhrRSJ6pWGdq5g5GRYXbs2EJmUkpdXVO3SHmeRxQBZERRQq1Wx5gE1zFYBrLUol6PGBsfo9hZbA40d2xsK22OA7Hc5mxSIiIiIjLnlFgcIG644QbWr1/PwUcfzrLnH027X6JklbCTFBoNUlLq1EnTlGKxiAlDKBYxNM/5rczC7XIha07sajC41LHIaGYKDrgJ2BmO0zy3D5NmL8Qkx3GwHbs505QNDhNjITKgsktjXSDgsRmpJk0MzK7Wqzyw9gHGx8bpG+qjEUUkfmuz6naUQrUK6fTRT0EQYEolRneOURutEEebGNw5ihNY9C7ppdzeHKg12DfIvb+/l8PnHcLqw4+htXUDlirliYiIiMw5JRYHiI0bN7Jx40ZObvFYdeKxBLaLkzpYaQaRISUlsRIMBs/zmm9qjqgmAGxjUyqXAIcKkFmT2UAGpvl88++8s2vsogE8fjbXgOasA/4uz030XMRJzPaBPsbGRqk0KqSpAbeE7XiQGYjj6RmNZeG6EzNUJYaoEhGZUaxaRMdBnZQ62vF8jyzLqIxV2LFlBwvDLjy3iOsG+WdJEBEREZH9RonFASauVqns3MFoVGH96DbKpRLd7e34oU+xtUiapjQauw9YTk3KYH0QsHDDENu2CQIf27Koj8RkcYOZzZG268JzxmVAA7IK1KpQr9sEQYgBGthgW2CHNLs59jy95lijwc5qlc6ODlp7eii1lykWi+zYsoOtG7aybcs2KpUKt97yex654Xfs2LadRDUsREREROacEosDTBrHROPjjMUV+it9dHZ14Yc2LV4LrW4rlmXRaEQ8fh7uzGRU4yoAJR8s28V1y9iWjZ1GZNFT6KnYC4PZ4zTglrGaPRuxRRJbpKlFGPpYtiGCierZHlg+u06ybE1sjTGGRpoyHkW0ex5BSwt+IcTzPMZHx3n04UcZGRkhiiIGNm7l9lvvzz/3s4iIiIjsV0osDjAFt0BPsYehoe08+MDDdPR2UqvVaG9vp1ar4fshhUI7ltXsB3i8qBGxY+MOsjSjq6uLMAxp72ynUCrSiCBLp+aR2uVN5LtDygECiOoRY5vHmr0ifoDjOwStQfPvMKCjA446ajljlTG2b99Eo1GHRgPbcWlb0kJXZydewcOyoLU1wBiLoS1VBgYGcF2X7u5uurtb6e4uMjowyLZHhtg2sI1KsYKJDcVakUbgP5aRiIiIiMicU2JxgAncgNaglbS6hS3rt1Kt1XCLLrVajSAIKJc7CMOeicRi956LNE7p29xH3IhJqymllhLtPe34LT5xBdLY0MwimpmEhdXMMfImFj7E1Zix/jEcx8GUDX7Bx2/xsRwLP/CxXItFi+YzOl5kcHB7M7GIYyxjKLUVaOlqwfWah16h4OLYDkkSMTo6iu04tLa20tpSpKXFZ+ejVbau38owwzT8BlZgEQQBrusqr5C5M5q3avNBOeOC/OvO+6udPHsKR95qnjuDaN0aOadMzv0fezNYaN7PNG9do7z/cszkX5goX9i2fGEjC9pyr9nkHGc47OdbZvv2kVxxW3NuMsDB9uZccV7O/bipsij/yvNWoY7zft4z2PBpA1GfSM67SkzefwBmMPZ0JlW6c1BicYAJw5DOzk7ahtsodhZwSj41YKRWY8uWLfT0JCxatATbdmg0CjSzggjLcgjDdhyrxIKlNeqVCsObtjO4NaURVQlbSgRBC67r4ZctHN/C9/3moOmSh1tym7/dCeABLiRJQrzL+AXHd/CLPrWxGrV6rVnLwtgYLIqEZDjUqINjaGtvIygELFu2jGq1SqVSwbZt5s2bR3tbO1k9o1qtMLhlO3FcZ6Cvr9kjUyrhhiG4E4lHsUBnZyfVoSq1gRrtVpmeznay8k4eVWohIiIicsBQYnGA8X2ftrY2Sm0lgvYQu+jRsCxMo0G2cydBEOB5GbbtEkU+xjQTi+Zg7TKum9F9UJ3qqM/mu+5nbGCEsVqdoFSit7eXYqlIiRJ+ycdxHFzXxQ3dZv2KGlCnOX9tCDQgqj6WmTuBQxAEOLZDo9HANjYuLjY+EEwU6xvHcWzKpTLFUpEoimg0GoyPj2NZFp2dnZSKJSo7KtQqdXbs2MT4+DDDg80q4E5Ly0Ri0RzcHQYh7e3tbB/eTmOkgdfayryOVkYKBaUVIiIiIgcQJRYHmPaWdg5deCjrhvropECc2tRrNQrlMgt6e+nq6mrWgyCj2cVgeOw2igjHhbbebsJyifkHHUTB9hnq76fStx2PBvXWFgZHPXAcDAYLi+7uVlpbS82u6dQFNwY3IZlYw+RUsI7j4Hkew30D1AYHCctFgt4ufIJpnW5ZljVnrsqgxW+h5JQol8pYlkVAQFbN2LZzG0NDQ6x/8B5GBndSKvUSBC0U24q0LuggLIWARcu8TopdnVTDmKqpcrDfwaHF+QyXNmuaWREREZEDiBKLA0xbuY2lBy1l4aZH6KDASJoxWm9gt7Uxb/58Ojs6sCwLYzKa3Qs2UKaZYIxjOzblnvmkUcK8BQsIEtj+0EOMDg/hly3qaRuVCkSRoVqtEkURixZ109PTSrOrIphYbr1ZJ6NQIAhDCoXCVJXr8R1D1IaGcLEITUCAz64jqY0xzR4NmrNZ2YHdbKINVsUiqkVs37mdrTu2cM99d9O/bTsrVx7HgoNaKbYV6ZjfTkAzb2jp6SBsb6dhRzTq4xxsWllKFxuL7SqLJyIiInIAUWJxgPn9rb/n4q9czFA0Rq/fQqEATjvM7+lh/uLFlAoFbNvGGEMQBGQZRFGDiQlbp07qszjFc11KpRKHLzmcRqVK1Q9I6g5eGmOTksVg16Gxo8pwJQN3HFyPJI5JkpieeV0cVOjATUK8ejjVY9ERunTOy7BbCtgFa69jkwzQsCbGBUURWAY/a87m5Ps+QRBgpw7UIa5DvWbYsWWASpxQICYkJezooNDRwc5t26jX69iFNtpa2igUCk/XRyIiIiIiOSixOMDcePON3HjzjRxzzDGcfvrptJRt/E5YOH8+Cw89tFlWbnwcyxjCMCRNU+J4HGOm9xZkcYrne7SUW5i/fBVEGffs2EF/rYpPBqSYCOwaRLUqg1SnxlbUaoZ6DbqcFhYeVIA0gLRA4AcUvSKEZcyCMrUC7Jjs5NhD98HUhLjGQKOBRYaDg0Vz4HghLOCkDtQgqkGtBrVHd0LfTgpUCWlMJRb1ep1arYZdsmnv6KBUKk31oIiIiIjI3FNicYDq6Ohg1apV9CWjjNe2kY1U2bl5C47JYHy8eZ+Q55FECZWdQxgDXrGIcWyqwyOQGToNYFncdecdDPUNsHVsjEocARGYlGg8ImnsMsWZT3M2qBjiGO4bHaS6dXPzSZozSPmex0Hz53PsqlVU05j+rQP4pRKFxa1g28SAZVl4nocxEFcrkIHnOdi2R+zZZHZGloKd2KxYsowlrd10zl9CsbWboeEdVEZG6Wov0tnSiue04poWjF8g88oMb+7j6puv5p677ybLnj3TaYqIiIg80ymxOEDNnz+fE044gQc2r2PDbdtJB8bZ8tBDWGkClQo4DpTLJNWIyro+LGzK8+aReg59jOPYFguKvTiWxW9+9Svuv/v+x82gtOfq2bvaZFncsofnTz75ZJ5/zDGYRoOtG7ZS6mxh/sE94DanxrUtGycMIcuoD22H1OA487Adn0YAicnIEnAbDs9bdTRtno9llTDG44EdO9jyaD8LwsNZ3HsIeM1ulKAAhdDwi9/+gsv/8XKyLJvqpRERERGRuafE4gC1bt06fvSjH5H5NvOKHTSShLGHtoExeGnarPNQyjBxilvLsC0D4+M4vkdPa0iWpNx9yx1U+wYZGhx6aifhxuwx9/B9n+7ubhqDBgbHadRjtt/7KLbrUAdwbJxyAMaQjo5gGYt6e4rjB1AGQ0qYZjiOy/r7H6I+NAr4GOOwbeNGhnbupF6vs3XLFvA88Dzc5v9j7X1rlVSIiIiIHICUWBygbrnlFm699VZOOukk3vmud9LXt4ONf7gby7Ka4ws8D0o1HNuiZMB2DDCIWwg4uOtQqo0a3/zeT9i4dh1pmu7TtpVKJRYvXkySpthbhqgmCQ9tGga7OeYhdWC8DJYNJdOcrrY8bxA3bCYWjmuxOCsQeB6/+O/fcNcf7ppatjHNAeiWZe1xDEVmlFTIM0jeCt1teSt0A/Nyxq3tzBk4nDNuJrceduSMG8oXdtf2Gaxb5lbeqsQz+Xcp729+PWdc3qrfMylJnLNyMvl+E9JN+X8T7EX59s8A+X4T6vODJw8CeucP5IoDGMu5f+K8n81o7lXP4Ew372c4k2M371jQvL+vOb9fab7PEHisYsE+osTiAGWMIUkS+vr6+N3/+x0jIyNs3LCxWQsiCLBcF4IA27IIaZ7EE4Lju2zbNkCjHjE6PLrPkwqALVu28JOf/IQ0TVm6ZAlJlk38nDcL5mVWxrhbBxvKYYjjOhCGWJ5LKSxhW7DlrrWM9vUz2D+4xzYqeRARERF5ZlFicYB78MEHefjhhwF2H6w8cUV/Wj5sMVHnAtIkb/Y9M7fddht/9Vd/xfOf/3w+/vGPYzsOO3fswMpsSpQwacxYpQ9si5b58zCOzY7qDjIylnQuIYsNn7r+O/z+lt+T7Kc2ioiIiMjTS4nFAS7LsgNu9qMsy6jX6/T19XHLLbdg2zYjIyOQQYECJkup1YbAhsLmzRjHYrg2TEbGjrY+TGIY6B8gjuO53hQRERER2UeUWMhT9tBDD/GZz3wGeNytS2bq/0z1qkwOA7et5j2r9Xree2FFRERE5JlAiYU8ZWmaUq1W57oZIiIiInIAmMmUByIiIiIiInukxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1Dd4WEZlrM6kHmbewcSHMF1dzci5wBtNe+4V8cVHOytvyDJK30nDeOMhf6TjncUfeY34mBWZzft9o2+errubc7tpoKVfc8CP5KnRvKR6SKw6Ag/OF+aVGvsCZzFbv5w3MWfXbylu5nRkU3s67QXmP8bz/UACN3DsoF/VYiIiIiIjIrCmxEBERERGRWVNiISIiIiIis6bEQkREREREZk2JhYiIiIiIzJoSCxERERERmTUlFiIiIiIiMmtKLEREREREZNaUWIiIiIiIyKwpsRARERERkVlz57oBIiJ/9Ea3ziB2/zVjn4lmsD3yLOPkjAv2/ardnMtMGjkXmOZfd2cxX1wt5/Jm8D2v3ernjcwZN54zrJBzecAD+WIjP+dnGOX9DIGdeY+1nDvdtOZfd95dVPVyBub9fiU54/Y99ViIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrKnytoiIiOwjOascz6TwdpYzOHfx4rwrn0EjrZxxOQt0z+iy78681Zj3ddXmMGcc5K7mHeX9EOszWHfeCuqlnHE5twXAKudcdc5jrZJ3xXn3I9CdPzQP9ViIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrKnytoiIiOwbeYs7N6J9v+7YzxeXt9Jw/wzWnbfydj3LF1eewXXfUs7YSt6K0Xk/mxlUd97nZlL1O6+8253zM4QZVMrOWUncyrndZiTvimGgI39sDuqxEBERERGRWVNiISIiIiIis6bEQkREREREZk2JhYiIiIiIzJoSCxERERERmTUlFiIiIiIiMmtKLEREREREZNaUWIiIiIiIyKwpsRARERERkVlTYiEiIiIiIrPmznUDRERE5FnC5A10ZrDQmcTmMLYfVlvPGxjnXF6Qf92tOeMqeU/5/JxxuT9s8l/HLu6Hdefc53g549L8q867yH3dxKicM5D8uzwn9ViIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya0osRERERERk1pRYiIiIiIjIrKnytoiIiOwbVt7AGVzXLOSMy1swupozbn8Uls5y7qBkBusenUHsvly5PYOSzdkMqlXnkvtAA7KccXnLX8/gw8m7asJ8Ybl3Y94S3czsWMtBPRYiIiIiIjJrSixERERERGTWlFiIiIiIiMisKbEQEREREZFZU2IhIiIiIiKzpsRCRERERERmTYmFiIiIiIjMmhILERERERGZNSUWIiIiIiIya6q8LSIiIvtG3oK/6QzKWls5qyznrbydt9JwZT+0MW8jnfyrzl85Oe/2jOVc3Awqb8+oUnYOMygsTZz3GnojZ9wMjos0bzXvnBu0rwuYQ/7Nzkk9FiIiIiIiMmtKLEREREREZNaUWIiIiIiIyKwpsRARERERkVlTYiEiIiIiIrOmxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1JRYiIiIiIjJruStvGzODSoMiIiLyxyd3xegZXNfMW7x4dF+vOsobCCbnQgs5KyzPpLJ03u0ZzluaPOfKTd4dPoNllnIubiaVydOc253lPc/Nux8hf5XunCW1rZwbbnKXY4cwdyqQi3osRERERERk1pRYiIiIiIjIrCmxEBERERGRWVNiISIiIiIis6bEQkREREREZk2JhYiIiIiIzJoSCxERERERmTUlFiIiIiIiMmtKLEREREREZNaUWIiIiIiIyKzt2zreIiIiIk8qyR8am3xxJS9fXDX/qvNz8oXVavnirEL+VTfyBqY5192ZL87EeVcMWPnC8h4WlRmsmpzHT96VF0v5V533WPNzxuXe5TM4vd/HXQzqsRARERERkVlTYiEiIiIiIrOmxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1JRYiIiIiIjJrSixERERERGTWlFiIiIiIiMisKbEQEREREZFZs4wxeUsSioiIiIiI7JF6LEREREREZNaUWIiIiIiIyKwpsRARERERkVlTYiEiIiIiIrOmxEJERERERGZNiYWIiIiIiMyaEgsREREREZk1JRYiIiIiIjJrSixERERERGTW/n9KDKzn5vDNwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original test image (without preprocessing)\n",
    "original_image = image.load_img(test_image_path)\n",
    "\n",
    "# Visualize the original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Visualize Grad-CAM heatmap\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(original_image, alpha=0.6)\n",
    "plt.imshow(gradcam_heatmap[1], cmap='jet', alpha=0.2)\n",
    "plt.title('Grad-CAM Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "# Visualize SmoothGrad heatmap\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(original_image, alpha=0.6)\n",
    "plt.imshow(smoothgrad_heatmap[0], cmap='jet', alpha=0.2)\n",
    "plt.title('SmoothGrad Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('6.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
